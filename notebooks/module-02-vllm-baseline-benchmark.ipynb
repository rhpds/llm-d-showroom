{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: vLLM Baseline Benchmarking\n",
    "\n",
    "This notebook runs GuideLLM benchmarks against your single-GPU vLLM deployment to establish a performance baseline for ParasolCloud's customer service workload.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook:\n",
    "- Your vLLM InferenceService (`llama-vllm-single`) should be deployed and in `Ready` state\n",
    "- Grafana dashboard should be configured to monitor vLLM metrics\n",
    "\n",
    "## Objectives\n",
    "\n",
    "By the end of this notebook, you'll have:\n",
    "- Established single-GPU performance baseline\n",
    "- Measured TTFT, ITL, and throughput at different load levels\n",
    "- Identified saturation point for single-GPU deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install GuideLLM if not already installed\n",
    "!pip install guidellm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output directory for results\n",
    "output_dir = Path(\"benchmark-results/module-02\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Results will be saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get InferenceService URL\n",
    "\n",
    "Retrieve the URL for your vLLM deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the inference URL from the InferenceService\n",
    "result = subprocess.run(\n",
    "    [\"oc\", \"get\", \"inferenceservice\", \"llama-vllm-single\", \"-o\", \"jsonpath='{.status.url}'\"],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "INFERENCE_URL = result.stdout.strip().strip(\"'\")\n",
    "print(f\"InferenceService URL: {INFERENCE_URL}\")\n",
    "\n",
    "# Verify the service is reachable\n",
    "if not INFERENCE_URL:\n",
    "    raise ValueError(\"Could not retrieve InferenceService URL. Ensure llama-vllm-single is deployed and Ready.\")\n",
    "\n",
    "# Construct the completions endpoint\n",
    "COMPLETIONS_URL = f\"{INFERENCE_URL}/v1/completions\"\n",
    "print(f\"Completions endpoint: {COMPLETIONS_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ParasolCloud Customer Service Dataset\n",
    "\n",
    "Generate a dataset that simulates ParasolCloud's customer service workload with shared system prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ParasolCloud customer service prompts with shared prefix\n",
    "customer_service_prompts = [\n",
    "    \"You are a helpful customer service agent. User asks: How do I reset my password?\",\n",
    "    \"You are a helpful customer service agent. User asks: What are your business hours?\",\n",
    "    \"You are a helpful customer service agent. User asks: How can I track my order?\",\n",
    "    \"You are a helpful customer service agent. User asks: What is your return policy?\",\n",
    "    \"You are a helpful customer service agent. User asks: How do I contact support?\",\n",
    "    \"You are a helpful customer service agent. User asks: Can I change my shipping address?\",\n",
    "    \"You are a helpful customer service agent. User asks: What payment methods do you accept?\",\n",
    "    \"You are a helpful customer service agent. User asks: How do I update my account information?\",\n",
    "    \"You are a helpful customer service agent. User asks: Where is my refund?\",\n",
    "    \"You are a helpful customer service agent. User asks: How long does shipping take?\",\n",
    "    \"You are a helpful customer service agent. User asks: Do you offer international shipping?\",\n",
    "    \"You are a helpful customer service agent. User asks: Can I cancel my order?\",\n",
    "    \"You are a helpful customer service agent. User asks: How do I create an account?\",\n",
    "    \"You are a helpful customer service agent. User asks: What is your privacy policy?\",\n",
    "    \"You are a helpful customer service agent. User asks: Do you have a mobile app?\",\n",
    "    \"You are a helpful customer service agent. User asks: How do I apply a discount code?\",\n",
    "]\n",
    "\n",
    "# Save to file for GuideLLM\n",
    "dataset_file = output_dir / \"customer-service-prompts.txt\"\n",
    "with open(dataset_file, 'w') as f:\n",
    "    f.write('\\n'.join(customer_service_prompts))\n",
    "\n",
    "print(f\"Created dataset with {len(customer_service_prompts)} prompts\")\n",
    "print(f\"Saved to: {dataset_file}\")\n",
    "print(f\"\\nShared prefix: 'You are a helpful customer service agent.'\")\n",
    "print(f\"This simulates ParasolCloud's workload where all requests share a system prompt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 1: Low Load (1 concurrent request)\n",
    "\n",
    "Understand single-request performance without queuing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Running low-load benchmark (1 concurrent request)...\")\n",
    "print(\"This will take approximately 2-3 minutes.\\n\")\n",
    "\n",
    "# Run GuideLLM benchmark\n",
    "subprocess.run([\n",
    "    \"guidellm\",\n",
    "    \"--target\", COMPLETIONS_URL,\n",
    "    \"--model\", \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"--data\", str(dataset_file),\n",
    "    \"--rate\", \"1\",\n",
    "    \"--max-requests\", \"50\",\n",
    "    \"--output-format\", \"json\",\n",
    "    \"--output-file\", str(output_dir / \"baseline-single-low.json\")\n",
    "], check=True)\n",
    "\n",
    "print(\"\\nâœ“ Low-load benchmark complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 2: Medium Load (5 concurrent requests)\n",
    "\n",
    "Increase concurrency to measure throughput scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Running medium-load benchmark (5 concurrent requests)...\")\n",
    "print(\"This will take approximately 3-4 minutes.\\n\")\n",
    "\n",
    "subprocess.run([\n",
    "    \"guidellm\",\n",
    "    \"--target\", COMPLETIONS_URL,\n",
    "    \"--model\", \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"--data\", str(dataset_file),\n",
    "    \"--rate\", \"5\",\n",
    "    \"--max-requests\", \"100\",\n",
    "    \"--output-format\", \"json\",\n",
    "    \"--output-file\", str(output_dir / \"baseline-single-medium.json\")\n",
    "], check=True)\n",
    "\n",
    "print(\"\\nâœ“ Medium-load benchmark complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 3: High Load (20 concurrent requests)\n",
    "\n",
    "Push to saturation to find maximum throughput:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Running high-load benchmark (20 concurrent requests)...\")\n",
    "print(\"This will take approximately 5-7 minutes.\\n\")\n",
    "print(\"ðŸ’¡ TIP: Open your Grafana dashboard now to watch metrics in real-time!\")\n",
    "\n",
    "subprocess.run([\n",
    "    \"guidellm\",\n",
    "    \"--target\", COMPLETIONS_URL,\n",
    "    \"--model\", \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"--data\", str(dataset_file),\n",
    "    \"--rate\", \"20\",\n",
    "    \"--max-requests\", \"200\",\n",
    "    \"--output-format\", \"json\",\n",
    "    \"--output-file\", str(output_dir / \"baseline-single-high.json\")\n",
    "], check=True)\n",
    "\n",
    "print(\"\\nâœ“ High-load benchmark complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Load and analyze the benchmark results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_benchmark_results(filepath):\n",
    "    \"\"\"Load GuideLLM JSON results.\"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def extract_metrics(results):\n",
    "    \"\"\"Extract key metrics from GuideLLM results.\"\"\"\n",
    "    summary = results.get('summary', {})\n",
    "    return {\n",
    "        'throughput': summary.get('throughput', 0),\n",
    "        'ttft_p50': summary.get('ttft_p50', 0),\n",
    "        'ttft_p95': summary.get('ttft_p95', 0),\n",
    "        'itl_p50': summary.get('itl_p50', 0),\n",
    "        'itl_p95': summary.get('itl_p95', 0),\n",
    "        'latency_p95': summary.get('latency_p95', 0),\n",
    "    }\n",
    "\n",
    "# Load all benchmark results\n",
    "low_results = load_benchmark_results(output_dir / \"baseline-single-low.json\")\n",
    "medium_results = load_benchmark_results(output_dir / \"baseline-single-medium.json\")\n",
    "high_results = load_benchmark_results(output_dir / \"baseline-single-high.json\")\n",
    "\n",
    "# Extract metrics\n",
    "low_metrics = extract_metrics(low_results)\n",
    "medium_metrics = extract_metrics(medium_results)\n",
    "high_metrics = extract_metrics(high_results)\n",
    "\n",
    "print(\"âœ“ Results loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame([\n",
    "    {'Load Level': 'Low (1 concurrent)', **low_metrics},\n",
    "    {'Load Level': 'Medium (5 concurrent)', **medium_metrics},\n",
    "    {'Load Level': 'High (20 concurrent)', **high_metrics},\n",
    "])\n",
    "\n",
    "# Format for display\n",
    "summary_df['throughput'] = summary_df['throughput'].round(2)\n",
    "summary_df['ttft_p50'] = (summary_df['ttft_p50'] * 1000).round(0).astype(int)  # Convert to ms\n",
    "summary_df['ttft_p95'] = (summary_df['ttft_p95'] * 1000).round(0).astype(int)\n",
    "summary_df['latency_p95'] = (summary_df['latency_p95'] * 1000).round(0).astype(int)\n",
    "\n",
    "# Rename columns for clarity\n",
    "summary_df = summary_df.rename(columns={\n",
    "    'throughput': 'Throughput (req/s)',\n",
    "    'ttft_p50': 'TTFT P50 (ms)',\n",
    "    'ttft_p95': 'TTFT P95 (ms)',\n",
    "    'latency_p95': 'P95 Latency (ms)'\n",
    "})\n",
    "\n",
    "print(\"\\n=== Single GPU vLLM Baseline Performance ===\")\n",
    "print(summary_df[['Load Level', 'Throughput (req/s)', 'TTFT P50 (ms)', 'TTFT P95 (ms)', 'P95 Latency (ms)']].to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv(output_dir / \"baseline-summary.csv\", index=False)\n",
    "print(f\"\\nâœ“ Summary saved to: {output_dir / 'baseline-summary.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Performance Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "load_levels = ['Low\\n(1)', 'Medium\\n(5)', 'High\\n(20)']\n",
    "throughput = summary_df['Throughput (req/s)'].values\n",
    "ttft_p95 = summary_df['TTFT P95 (ms)'].values\n",
    "\n",
    "# Throughput plot\n",
    "axes[0].bar(load_levels, throughput, color=['green', 'orange', 'red'])\n",
    "axes[0].set_ylabel('Throughput (req/s)', fontsize=12)\n",
    "axes[0].set_xlabel('Concurrency Level', fontsize=12)\n",
    "axes[0].set_title('Single GPU Throughput vs. Load', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(throughput):\n",
    "    axes[0].text(i, v + 1, f'{v:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# TTFT P95 latency plot\n",
    "axes[1].bar(load_levels, ttft_p95, color=['green', 'orange', 'red'])\n",
    "axes[1].set_ylabel('TTFT P95 (ms)', fontsize=12)\n",
    "axes[1].set_xlabel('Concurrency Level', fontsize=12)\n",
    "axes[1].set_title('Single GPU TTFT P95 vs. Load', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(ttft_p95):\n",
    "    axes[1].text(i, v + 10, f'{v:.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'baseline-performance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Chart saved to: {output_dir / 'baseline-performance.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Observations\n",
    "\n",
    "Document your findings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_throughput = high_metrics['throughput']\n",
    "ttft_p95_high = high_metrics['ttft_p95'] * 1000  # Convert to ms\n",
    "parasolcloud_target = 500  # req/s\n",
    "gpus_needed_naive = parasolcloud_target / max_throughput\n",
    "\n",
    "observations = f\"\"\"\n",
    "=== Single GPU vLLM Baseline Observations ===\n",
    "\n",
    "Configuration:\n",
    "- GPUs: 1\n",
    "- Model: Meta-Llama-3.1-8B-Instruct\n",
    "- Max model length: 4096 tokens\n",
    "- GPU memory utilization: 0.9\n",
    "\n",
    "Performance Metrics (High Load - 20 concurrent):\n",
    "- Maximum Throughput: ~{max_throughput:.1f} req/s\n",
    "- TTFT (P95): ~{ttft_p95_high:.0f}ms\n",
    "- P95 Latency: ~{high_metrics['latency_p95'] * 1000:.0f}ms\n",
    "\n",
    "Key Observations:\n",
    "- Single GPU saturates at ~{max_throughput:.0f} req/s\n",
    "- Latency increases significantly under load\n",
    "- No cache sharing across requests (every request recomputes full prompt)\n",
    "\n",
    "ParasolCloud's Challenge:\n",
    "- Current capacity: {max_throughput:.0f} req/s per GPU\n",
    "- Target capacity: {parasolcloud_target}+ req/s\n",
    "- Naive calculation: Need ~{gpus_needed_naive:.0f} GPUs for {parasolcloud_target/max_throughput:.0f}x scaling\n",
    "- Question: Can intelligent orchestration improve this ratio?\n",
    "\"\"\"\n",
    "\n",
    "print(observations)\n",
    "\n",
    "# Save observations\n",
    "with open(output_dir / 'baseline-observations.txt', 'w') as f:\n",
    "    f.write(observations)\n",
    "\n",
    "print(f\"\\nâœ“ Observations saved to: {output_dir / 'baseline-observations.txt'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "You've established your single-GPU baseline! Key findings:\n",
    "\n",
    "1. **Maximum throughput**: The single GPU saturates at a specific req/s rate\n",
    "2. **Latency degradation**: TTFT increases significantly as load approaches saturation\n",
    "3. **No cache sharing**: Each request recomputes the shared \"You are a helpful customer service agent\" prefix\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 3**, you'll:\n",
    "- Scale to 4 GPUs using naive horizontal scaling (round-robin load balancing)\n",
    "- Run the same benchmarks\n",
    "- Discover why simple scaling doesn't deliver linear 4x improvement\n",
    "- Understand the impact of isolated KV caches (no sharing)\n",
    "\n",
    "### Before You Go\n",
    "\n",
    "âœ“ Review your Grafana dashboard - take screenshots of:\n",
    "- Peak throughput (requests per second)\n",
    "- GPU cache usage\n",
    "- Request latency distribution\n",
    "\n",
    "âœ“ Keep your benchmark results - you'll compare them against scaled deployments in later modules"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
