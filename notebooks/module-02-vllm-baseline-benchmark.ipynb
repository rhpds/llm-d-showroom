{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Module 2: vLLM Baseline Tail Latency Benchmarking\n\nThis notebook runs GuideLLM benchmarks against your single-GPU vLLM deployment to establish a **tail latency baseline** (P95/P99) for ParasolCloud's customer service workload.\n\n## Prerequisites\n\nBefore running this notebook:\n- Your vLLM InferenceService (`llama-vllm-single`) should be deployed and in `Ready` state\n- Grafana dashboard should be configured to monitor vLLM metrics\n\n## Objectives\n\nBy the end of this notebook, you'll have:\n- Established single-GPU **P95/P99 tail latency baseline** across two critical scenarios\n- **Scenario 1**: Single-turn requests with large shared prefixes (prefill bottleneck)\n- **Scenario 2**: Multi-turn chat conversations (cache reuse opportunity)\n- Identified how tail latency degrades under load\n- Documented what your most frustrated users currently experience"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install GuideLLM if not already installed\n",
    "!pip install guidellm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output directory for results\n",
    "output_dir = Path(\"benchmark-results/module-02\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Results will be saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get InferenceService URL\n",
    "\n",
    "Retrieve the URL for your vLLM deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the inference URL from the InferenceService\n",
    "result = subprocess.run(\n",
    "    [\"oc\", \"get\", \"inferenceservice\", \"llama-vllm-single\", \"-o\", \"jsonpath='{.status.url}'\"],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "INFERENCE_URL = result.stdout.strip().strip(\"'\")\n",
    "print(f\"InferenceService URL: {INFERENCE_URL}\")\n",
    "\n",
    "# Verify the service is reachable\n",
    "if not INFERENCE_URL:\n",
    "    raise ValueError(\"Could not retrieve InferenceService URL. Ensure llama-vllm-single is deployed and Ready.\")\n",
    "\n",
    "# Construct the completions endpoint\n",
    "COMPLETIONS_URL = f\"{INFERENCE_URL}/v1/completions\"\n",
    "print(f\"Completions endpoint: {COMPLETIONS_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Create ParasolCloud Customer Service Datasets\n\nGenerate two datasets to test different tail latency scenarios:\n\n**Scenario 1: Single-turn with large shared prefixes**\n- All requests share a 300-token system prompt\n- Tests prefill bottleneck and cache miss impact\n\n**Scenario 2: Multi-turn chat conversations**\n- Simulates ongoing customer conversations\n- Tests KV cache reuse across conversation turns"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Scenario 1: Single-turn requests with large shared prefixes\n# All requests share the same system prompt - testing cache miss impact\n\n# Create a longer system prompt (approximately 300 tokens)\nsystem_prompt = \"\"\"You are an expert customer service agent for ParasolCloud, a leading enterprise cloud infrastructure provider. \nYour role is to assist customers with technical issues, account questions, billing inquiries, and service recommendations. \nYou should be professional, empathetic, and solution-oriented. Always prioritize customer satisfaction while following company policies. \nWhen addressing technical issues, provide clear step-by-step instructions. For billing questions, explain charges clearly and offer to escalate complex cases. \nIf you don't know an answer, be honest and offer to connect the customer with a specialist. Remember to maintain a friendly, helpful tone throughout the interaction.\"\"\"\n\nsingle_turn_prompts = [\n    f\"{system_prompt}\\n\\nCustomer asks: How do I reset my account password?\",\n    f\"{system_prompt}\\n\\nCustomer asks: What are your business support hours?\",\n    f\"{system_prompt}\\n\\nCustomer asks: How can I track my open support tickets?\",\n    f\"{system_prompt}\\n\\nCustomer asks: What is your data retention policy?\",\n    f\"{system_prompt}\\n\\nCustomer asks: How do I contact the billing department?\",\n    f\"{system_prompt}\\n\\nCustomer asks: Can I change my subscription tier mid-cycle?\",\n    f\"{system_prompt}\\n\\nCustomer asks: What payment methods do you accept?\",\n    f\"{system_prompt}\\n\\nCustomer asks: How do I update my account contact information?\",\n    f\"{system_prompt}\\n\\nCustomer asks: Where can I find documentation for your API?\",\n    f\"{system_prompt}\\n\\nCustomer asks: How long does it take to process a refund?\",\n    f\"{system_prompt}\\n\\nCustomer asks: Do you offer enterprise-level SLAs?\",\n    f\"{system_prompt}\\n\\nCustomer asks: Can I cancel my service without penalties?\",\n    f\"{system_prompt}\\n\\nCustomer asks: How do I enable two-factor authentication?\",\n    f\"{system_prompt}\\n\\nCustomer asks: What is your service uptime guarantee?\",\n    f\"{system_prompt}\\n\\nCustomer asks: Do you have a mobile app for account management?\",\n    f\"{system_prompt}\\n\\nCustomer asks: How do I apply my discount code to my account?\",\n]\n\n# Scenario 2: Multi-turn chat conversations\n# Simulates ongoing customer conversations with growing context\n\nmulti_turn_conversations = [\n    # Conversation 1\n    f\"{system_prompt}\\n\\nCustomer: I'm having trouble logging into my account.\",\n    f\"{system_prompt}\\n\\nCustomer: I'm having trouble logging into my account.\\nAgent: I'd be happy to help. Have you tried resetting your password?\\nCustomer: No, how do I do that?\",\n    f\"{system_prompt}\\n\\nCustomer: I'm having trouble logging into my account.\\nAgent: I'd be happy to help. Have you tried resetting your password?\\nCustomer: No, how do I do that?\\nAgent: You can click the 'Forgot Password' link on the login page.\\nCustomer: I don't see that link. Where exactly is it?\",\n    \n    # Conversation 2\n    f\"{system_prompt}\\n\\nCustomer: What's included in the enterprise plan?\",\n    f\"{system_prompt}\\n\\nCustomer: What's included in the enterprise plan?\\nAgent: The enterprise plan includes 24/7 support, dedicated account manager, and 99.99% uptime SLA.\\nCustomer: What about data backup?\",\n    f\"{system_prompt}\\n\\nCustomer: What's included in the enterprise plan?\\nAgent: The enterprise plan includes 24/7 support, dedicated account manager, and 99.99% uptime SLA.\\nCustomer: What about data backup?\\nAgent: Yes, automated daily backups with 30-day retention are included.\\nCustomer: Can I increase the retention period?\",\n    \n    # Conversation 3\n    f\"{system_prompt}\\n\\nCustomer: I was charged twice this month.\",\n    f\"{system_prompt}\\n\\nCustomer: I was charged twice this month.\\nAgent: I apologize for the inconvenience. Let me look into this for you. Can you provide your account number?\\nCustomer: It's ACC-12345678.\",\n    f\"{system_prompt}\\n\\nCustomer: I was charged twice this month.\\nAgent: I apologize for the inconvenience. Let me look into this for you. Can you provide your account number?\\nCustomer: It's ACC-12345678.\\nAgent: Thank you. I see the duplicate charge. I'm processing a refund now.\\nCustomer: How long until I see the refund?\",\n]\n\n# Save both datasets\ndataset_dir = output_dir / \"datasets\"\ndataset_dir.mkdir(exist_ok=True)\n\nsingle_turn_file = dataset_dir / \"single-turn-prompts.txt\"\nwith open(single_turn_file, 'w') as f:\n    f.write('\\n'.join(single_turn_prompts))\n\nmulti_turn_file = dataset_dir / \"multi-turn-prompts.txt\"\nwith open(multi_turn_file, 'w') as f:\n    f.write('\\n'.join(multi_turn_conversations))\n\nprint(f\"âœ“ Created Scenario 1 dataset: {len(single_turn_prompts)} single-turn prompts with shared prefix\")\nprint(f\"  System prompt length: ~{len(system_prompt.split())} words (~300 tokens)\")\nprint(f\"  Saved to: {single_turn_file}\\n\")\n\nprint(f\"âœ“ Created Scenario 2 dataset: {len(multi_turn_conversations)} multi-turn conversation turns\")\nprint(f\"  Tests growing context across conversation turns\")\nprint(f\"  Saved to: {multi_turn_file}\\n\")\n\nprint(\"These scenarios test two critical tail latency bottlenecks:\")\nprint(\"  1. Prefill queuing from large shared prefixes (single-turn)\")\nprint(\"  2. Cache misses in multi-turn conversations (no KV cache reuse)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Scenario 1 Benchmarks: Single-turn with Large Shared Prefixes\n\nTest tail latency impact of prefill queuing with shared system prompts.\n\nWe'll run three load levels:\n- **Low load** (1 concurrent): Baseline P95/P99 without queuing\n- **Medium load** (5 concurrent): Moderate queuing - observe P95 degradation\n- **High load** (20 concurrent): Heavy queuing - measure worst-case P95/P99"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%time\nprint(\"Running Scenario 1 - Low load (1 concurrent request)...\")\nprint(\"This will take approximately 2-3 minutes.\\n\")\n\n# Run GuideLLM benchmark\nsubprocess.run([\n    \"guidellm\",\n    \"--target\", COMPLETIONS_URL,\n    \"--model\", \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n    \"--data\", str(single_turn_file),\n    \"--rate\", \"1\",\n    \"--max-requests\", \"50\",\n    \"--output-format\", \"json\",\n    \"--output-file\", str(output_dir / \"scenario1-single-turn-low.json\")\n], check=True)\n\nprint(\"\\nâœ“ Scenario 1 low-load benchmark complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Scenario 1 - Medium Load"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%time\nprint(\"Running Scenario 1 - Medium load (5 concurrent requests)...\")\nprint(\"This will take approximately 3-4 minutes.\\n\")\n\nsubprocess.run([\n    \"guidellm\",\n    \"--target\", COMPLETIONS_URL,\n    \"--model\", \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n    \"--data\", str(single_turn_file),\n    \"--rate\", \"5\",\n    \"--max-requests\", \"100\",\n    \"--output-format\", \"json\",\n    \"--output-file\", str(output_dir / \"scenario1-single-turn-medium.json\")\n], check=True)\n\nprint(\"\\nâœ“ Scenario 1 medium-load benchmark complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Scenario 1 - High Load"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%time\nprint(\"Running Scenario 1 - High load (20 concurrent requests)...\")\nprint(\"This will take approximately 5-7 minutes.\\n\")\nprint(\"ðŸ’¡ TIP: Open your Grafana dashboard now to watch metrics in real-time!\")\n\nsubprocess.run([\n    \"guidellm\",\n    \"--target\", COMPLETIONS_URL,\n    \"--model\", \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n    \"--data\", str(single_turn_file),\n    \"--rate\", \"20\",\n    \"--max-requests\", \"200\",\n    \"--output-format\", \"json\",\n    \"--output-file\", str(output_dir / \"scenario1-single-turn-high.json\")\n], check=True)\n\nprint(\"\\nâœ“ Scenario 1 high-load benchmark complete\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Scenario 2 Benchmarks: Multi-turn Chat Conversations\n\nTest tail latency impact of cache misses in multi-turn conversations.\n\nWithout intelligent routing, conversation turns land on random vLLM instances, causing cache misses and redundant computation of conversation history.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "%%time\nprint(\"Running Scenario 2 - Multi-turn chat (medium load)...\")\nprint(\"This tests conversation latency with growing context.\")\nprint(\"This will take approximately 3-4 minutes.\\n\")\n\nsubprocess.run([\n    \"guidellm\",\n    \"--target\", COMPLETIONS_URL,\n    \"--model\", \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n    \"--data\", str(multi_turn_file),\n    \"--rate\", \"5\",\n    \"--max-requests\", \"50\",\n    \"--output-format\", \"json\",\n    \"--output-file\", str(output_dir / \"scenario2-multi-turn.json\")\n], check=True)\n\nprint(\"\\nâœ“ Scenario 2 multi-turn benchmark complete\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Load and analyze the benchmark results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_benchmark_results(filepath):\n",
    "    \"\"\"Load GuideLLM JSON results.\"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def extract_metrics(results):\n",
    "    \"\"\"Extract key metrics from GuideLLM results.\"\"\"\n",
    "    summary = results.get('summary', {})\n",
    "    return {\n",
    "        'throughput': summary.get('throughput', 0),\n",
    "        'ttft_p50': summary.get('ttft_p50', 0),\n",
    "        'ttft_p95': summary.get('ttft_p95', 0),\n",
    "        'itl_p50': summary.get('itl_p50', 0),\n",
    "        'itl_p95': summary.get('itl_p95', 0),\n",
    "        'latency_p95': summary.get('latency_p95', 0),\n",
    "    }\n",
    "\n",
    "# Load all benchmark results\n",
    "low_results = load_benchmark_results(output_dir / \"baseline-single-low.json\")\n",
    "medium_results = load_benchmark_results(output_dir / \"baseline-single-medium.json\")\n",
    "high_results = load_benchmark_results(output_dir / \"baseline-single-high.json\")\n",
    "\n",
    "# Extract metrics\n",
    "low_metrics = extract_metrics(low_results)\n",
    "medium_metrics = extract_metrics(medium_results)\n",
    "high_metrics = extract_metrics(high_results)\n",
    "\n",
    "print(\"âœ“ Results loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame([\n",
    "    {'Load Level': 'Low (1 concurrent)', **low_metrics},\n",
    "    {'Load Level': 'Medium (5 concurrent)', **medium_metrics},\n",
    "    {'Load Level': 'High (20 concurrent)', **high_metrics},\n",
    "])\n",
    "\n",
    "# Format for display\n",
    "summary_df['throughput'] = summary_df['throughput'].round(2)\n",
    "summary_df['ttft_p50'] = (summary_df['ttft_p50'] * 1000).round(0).astype(int)  # Convert to ms\n",
    "summary_df['ttft_p95'] = (summary_df['ttft_p95'] * 1000).round(0).astype(int)\n",
    "summary_df['latency_p95'] = (summary_df['latency_p95'] * 1000).round(0).astype(int)\n",
    "\n",
    "# Rename columns for clarity\n",
    "summary_df = summary_df.rename(columns={\n",
    "    'throughput': 'Throughput (req/s)',\n",
    "    'ttft_p50': 'TTFT P50 (ms)',\n",
    "    'ttft_p95': 'TTFT P95 (ms)',\n",
    "    'latency_p95': 'P95 Latency (ms)'\n",
    "})\n",
    "\n",
    "print(\"\\n=== Single GPU vLLM Baseline Performance ===\")\n",
    "print(summary_df[['Load Level', 'Throughput (req/s)', 'TTFT P50 (ms)', 'TTFT P95 (ms)', 'P95 Latency (ms)']].to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv(output_dir / \"baseline-summary.csv\", index=False)\n",
    "print(f\"\\nâœ“ Summary saved to: {output_dir / 'baseline-summary.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Performance Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "load_levels = ['Low\\n(1)', 'Medium\\n(5)', 'High\\n(20)']\n",
    "throughput = summary_df['Throughput (req/s)'].values\n",
    "ttft_p95 = summary_df['TTFT P95 (ms)'].values\n",
    "\n",
    "# Throughput plot\n",
    "axes[0].bar(load_levels, throughput, color=['green', 'orange', 'red'])\n",
    "axes[0].set_ylabel('Throughput (req/s)', fontsize=12)\n",
    "axes[0].set_xlabel('Concurrency Level', fontsize=12)\n",
    "axes[0].set_title('Single GPU Throughput vs. Load', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(throughput):\n",
    "    axes[0].text(i, v + 1, f'{v:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# TTFT P95 latency plot\n",
    "axes[1].bar(load_levels, ttft_p95, color=['green', 'orange', 'red'])\n",
    "axes[1].set_ylabel('TTFT P95 (ms)', fontsize=12)\n",
    "axes[1].set_xlabel('Concurrency Level', fontsize=12)\n",
    "axes[1].set_title('Single GPU TTFT P95 vs. Load', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(ttft_p95):\n",
    "    axes[1].text(i, v + 10, f'{v:.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'baseline-performance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Chart saved to: {output_dir / 'baseline-performance.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Observations\n",
    "\n",
    "Document your findings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_throughput = high_metrics['throughput']\n",
    "ttft_p95_high = high_metrics['ttft_p95'] * 1000  # Convert to ms\n",
    "parasolcloud_target = 500  # req/s\n",
    "gpus_needed_naive = parasolcloud_target / max_throughput\n",
    "\n",
    "observations = f\"\"\"\n",
    "=== Single GPU vLLM Baseline Observations ===\n",
    "\n",
    "Configuration:\n",
    "- GPUs: 1\n",
    "- Model: Meta-Llama-3.1-8B-Instruct\n",
    "- Max model length: 4096 tokens\n",
    "- GPU memory utilization: 0.9\n",
    "\n",
    "Performance Metrics (High Load - 20 concurrent):\n",
    "- Maximum Throughput: ~{max_throughput:.1f} req/s\n",
    "- TTFT (P95): ~{ttft_p95_high:.0f}ms\n",
    "- P95 Latency: ~{high_metrics['latency_p95'] * 1000:.0f}ms\n",
    "\n",
    "Key Observations:\n",
    "- Single GPU saturates at ~{max_throughput:.0f} req/s\n",
    "- Latency increases significantly under load\n",
    "- No cache sharing across requests (every request recomputes full prompt)\n",
    "\n",
    "ParasolCloud's Challenge:\n",
    "- Current capacity: {max_throughput:.0f} req/s per GPU\n",
    "- Target capacity: {parasolcloud_target}+ req/s\n",
    "- Naive calculation: Need ~{gpus_needed_naive:.0f} GPUs for {parasolcloud_target/max_throughput:.0f}x scaling\n",
    "- Question: Can intelligent orchestration improve this ratio?\n",
    "\"\"\"\n",
    "\n",
    "print(observations)\n",
    "\n",
    "# Save observations\n",
    "with open(output_dir / 'baseline-observations.txt', 'w') as f:\n",
    "    f.write(observations)\n",
    "\n",
    "print(f\"\\nâœ“ Observations saved to: {output_dir / 'baseline-observations.txt'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "You've established your single-GPU baseline! Key findings:\n",
    "\n",
    "1. **Maximum throughput**: The single GPU saturates at a specific req/s rate\n",
    "2. **Latency degradation**: TTFT increases significantly as load approaches saturation\n",
    "3. **No cache sharing**: Each request recomputes the shared \"You are a helpful customer service agent\" prefix\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 3**, you'll:\n",
    "- Scale to 4 GPUs using naive horizontal scaling (round-robin load balancing)\n",
    "- Run the same benchmarks\n",
    "- Discover why simple scaling doesn't deliver linear 4x improvement\n",
    "- Understand the impact of isolated KV caches (no sharing)\n",
    "\n",
    "### Before You Go\n",
    "\n",
    "âœ“ Review your Grafana dashboard - take screenshots of:\n",
    "- Peak throughput (requests per second)\n",
    "- GPU cache usage\n",
    "- Request latency distribution\n",
    "\n",
    "âœ“ Keep your benchmark results - you'll compare them against scaled deployments in later modules"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}