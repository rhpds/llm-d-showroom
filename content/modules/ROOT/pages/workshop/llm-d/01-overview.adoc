= Workshop Overview: The ParasolCloud Challenge
:toc:
:toc-placement: preamble
:icons: font

== Your role and goal

You're a Platform Engineer at ParasolCloud, and you've been assigned to evaluate distributed LLM inference solutions for a critical upcoming deployment.

ParasolCloud needs to scale its AI-powered customer service platform significantly to handle increased traffic for a major client expansion. Your manager has explained: "We need to serve large language models efficiently across multiple GPUs. Our current single-instance vLLM deployment won't cut it for the traffic we're expecting. We need you to evaluate llm-d to see if it can give us the performance and cost efficiency we need."

**Your assignment**: Evaluate llm-d's distributed inference capabilities and demonstrate measurable performance improvements over naive vLLM scaling for ParasolCloud's production AI platform.

**The objective**: Provide clear benchmarks showing throughput and latency improvements that justify the architectural shift to llm-d-orchestrated distributed inference.

== ParasolCloud's infrastructure challenges

**The situation**: ParasolCloud needs to scale its AI-powered customer service platform from 500 concurrent users to 5,000+ concurrent users for a major enterprise client expansion launching in 90 days.

**Project timeline**: 90-day evaluation and deployment window for distributed inference architecture assessment and production rollout.

**Current challenges** (operational pain points):

* **Single-instance bottleneck**: Current vLLM deployment on single GPU cannot support the projected load required for client expansion
* **Poor cache utilization**: Similar customer queries don't benefit from KV cache sharing → Redundant computation wastes GPU cycles and increases latency
* **Unpredictable latency**: No intelligent request routing → High tail latencies impact customer experience during peak periods
* **Scaling uncertainty**: Adding more vLLM instances improves throughput but with diminishing returns → Unclear cost-to-performance ratio for required scale

**The opportunity**: llm-d technology can address these challenges through intelligent inference scheduling, and you've been selected to evaluate its feasibility for ParasolCloud's production deployment.

**Technical perspective**: "llm-d's Precise Prefix Cache Aware Routing has proven benefits in distributed inference environments, but I need to validate how it performs with our specific workload patterns and infrastructure constraints. The key question is: can we achieve significant throughput improvement without proportional GPU investment?"

== Your vision of success

If llm-d proves effective for ParasolCloud's use case, here's the potential technical improvements:

**Immediate improvements** (within 90-day evaluation window):
* **Intelligent request routing**: Prefix-aware scheduling → Better cache hit rates and reduced redundant computation
* **Throughput scaling**: Distributed inference across multiple GPUs → Sustained throughput with predictable latency

**Strategic benefits** (6-12 month horizon):
* **Cost efficiency**: Better GPU utilization → Support increased traffic with less than proportional hardware investment
* **Latency optimization**: Cache-aware routing and prefill/decode separation → Reduced P95 latency
* **Operational simplicity**: Single llm-d control plane → Simplified deployment, monitoring, and scaling versus managing many independent vLLM instances

**Success metric**: Demonstrable throughput improvement per GPU through intelligent scheduling, enabling ParasolCloud to support client expansion with manageable infrastructure investment and predictable cost model.

== What you'll deliver

By the end of this workshop, you'll present a recommendation to ParasolCloud leadership that includes:

* **Baseline performance data** - Single GPU vLLM deployment benchmarks
* **Naive scaling analysis** - Multi-GPU deployment with round-robin load balancing
* **llm-d performance comparison** - Intelligent orchestration with cache-aware routing
* **Business case** - GPU cost savings, latency improvements, and ROI quantification
* **Implementation roadmap** - Deployment timeline and production considerations

Your goal is to provide ParasolCloud with a clear, data-driven decision on whether llm-d can deliver the required scale within the 90-day timeline while minimizing infrastructure investment.
