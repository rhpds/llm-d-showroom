= Workshop Overview: The ParasolCloud Challenge
:toc:
:toc-placement: preamble
:icons: font

== Your role and goal

You're a Platform Engineer at ParasolCloud, and you've been assigned to evaluate distributed LLM inference solutions for a critical upcoming deployment.

ParasolCloud needs to improve its AI-powered customer service platform's response times to handle a major client expansion. Your manager has explained: "We're getting complaints about slow responses during peak hours—our most frustrated customers are waiting too long. Our current single-instance vLLM deployment isn't delivering consistent performance. We need you to evaluate llm-d to see if it can reduce tail latency and give our worst-case users a better experience."

**Your assignment**: Evaluate llm-d's cache-aware routing and demonstrate measurable tail latency (P95/P99) improvements over naive vLLM scaling for ParasolCloud's production AI platform.

**The objective**: Provide clear benchmarks showing P95 and P99 latency reductions that justify the architectural shift to llm-d-orchestrated distributed inference.

**Current challenges** (operational pain points):

* **Unpredictable tail latency**: No intelligent request routing → Your most frustrated users (P95/P99) experience unacceptably slow responses during peak periods
* **Prefill queuing bottleneck**: Large prompts cause head-of-line blocking → Some customers wait significantly longer than others for first response
* **Multi-turn chat inefficiency**: Conversation follow-ups recompute entire context every time → Unnecessary delays for ongoing customer interactions
* **Poor cache utilization**: Similar customer queries don't benefit from KV cache sharing → Redundant computation increases P95/P99 latency
* **Naive scaling doesn't help tail latency**: Adding more isolated vLLM instances improves capacity but doesn't reduce worst-case response times

**The opportunity**: llm-d technology can address these challenges through cache-aware routing and intelligent inference scheduling, and you've been selected to evaluate its feasibility for ParasolCloud's production deployment.

**Technical perspective**: "llm-d's Prefix Cache Aware Routing has proven benefits in distributed inference environments, but I need to validate how it performs with our specific workload patterns and infrastructure constraints. The key question is: can we dramatically reduce P95 and P99 latency to improve the experience for our most frustrated customers?"

== What you'll deliver

* **Baseline tail latency data** - Single GPU vLLM P95/P99 measurements across different scenarios
* **Naive scaling analysis** - Multi-GPU deployment showing that more GPUs don't fix tail latency
* **llm-d performance comparison** - Cache-aware routing dramatically reducing P95/P99 latency
* **Customer experience impact** - Quantified improvements for your most frustrated users
* **Implementation roadmap** - Deployment timeline and production considerations

Your goal is to provide ParasolCloud with a clear, data-driven decision on whether llm-d can deliver consistently low latency for all customers—especially those currently experiencing the worst performance.
