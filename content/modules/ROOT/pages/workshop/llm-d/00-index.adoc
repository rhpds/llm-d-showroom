= Introducing llm-d: Your Production-Ready Path to Scalable LLM Inference
:source-highlighter: rouge
:toc: macro
:toclevels: 2

toc::[]

Welcome to the llm-d workshop! In the next 2 hours, you'll gain hands-on experience with production-ready distributed LLM inference that dramatically improves tail latency—the experience your most frustrated users see.

== What you'll learn

This workshop teaches you how to reduce P95 and P99 latency using llm-d—an intelligent orchestration layer that sits above vLLM and routes requests based on KV cache awareness.

You'll work through a realistic business scenario as a Platform Engineer at ParasolCloud, tasked with improving customer experience for an AI-powered support platform. While naive scaling adds capacity, it doesn't solve the real problem: unpredictable, high tail latencies that frustrate users during peak periods.

**Your goal**: Evaluate whether llm-d can deliver consistent, low-latency responses—especially for the users currently experiencing the worst performance (P95/P99).

=== Hands-on skills you'll gain

By the end of this workshop, you'll be able to:

* **Deploy and benchmark LLM inference** using vLLM, KServe, and OpenShift AI
* **Measure tail latency scientifically** with GuideLLM benchmarks (P95, P99, TTFT)
* **Identify latency bottlenecks** - prefill queuing, KV cache misses, multi-turn chat inefficiency
* **Compare scaling strategies** - naive horizontal scaling vs. intelligent cache-aware routing
* **Deploy llm-d** with Precise Prefix Cache Aware Routing
* **Quantify customer experience improvements** - connect P95/P99 reductions to user satisfaction
* **Articulate value** in terms users understand: "faster responses for your most frustrated customers"

=== What makes this workshop different

This is NOT a lecture-heavy introduction to distributed systems. You'll spend:

* **20 minutes** on concepts (Module 1)
* **100 minutes** deploying, benchmarking, and analyzing real systems (Modules 2-5)

You'll generate real performance data, make architectural decisions based on benchmarks, and build a business case with quantified ROI.

== Workshop modules

=== Module 1: Introduction & Core Concepts (20 minutes)

Understand what causes tail latency and how to fix it:

* **Tail latency explained**: What P95/P99 means and why it matters ("your most frustrated users")
* **Latency bottlenecks**: Prefill queuing, KV cache misses, multi-turn chat inefficiency
* **LLM inference phases**: Prefill vs. decode and their different latency characteristics
* **KV cache sharing**: Why cache hits eliminate queuing and reduce P95
* **llm-d capabilities**: Intelligent scheduling to reduce tail latency

**Outcome**: Understanding that scaling for capacity ≠ scaling for latency.

=== Module 2: Deploy Model with vLLM & Benchmark (25 minutes)

Establish your tail latency baseline:

* Deploy Llama 3.1 8B Instruct on a single GPU using KServe and vLLM
* Configure Grafana dashboard for real-time latency metrics
* Run GuideLLM benchmarks from the terminal
* Measure P50, P95, P99 latency under load

**Outcome**: Baseline showing where tail latency problems emerge.

=== Module 3: Scale vLLM & Compare Performance (25 minutes)

Discover why naive scaling doesn't fix tail latency:

* Deploy vLLM with 4 replicas (1 per GPU) using InferenceService
* Observe round-robin load balancing with isolated KV caches
* Run the same benchmark and compare P95/P99 latency
* Compare P95/P99 latency: Does adding GPUs help your worst-case users?

**Outcome**: Understanding that more GPUs ≠ better P95/P99 (isolated caches = wasted prefill work).

=== Module 4: Deploy llm-d & Benchmark (35 minutes)

Fix tail latency with intelligent cache-aware routing:

* Deploy llm-d with Precise Prefix Cache Aware Routing using LLMInferenceService
* Configure llm-d to manage 4 vLLM backend instances
* Run the same benchmark
* **Measure P95/P99 improvements**: How much better is the experience for your most frustrated users?
* Observe cache hit rates and routing decisions in Grafana

**Outcome**: Demonstrable proof that intelligent routing reduces tail latency for shared-prefix workloads.


== What you'll demonstrate

By the end of this workshop, you'll have:

* **Deployed three configurations** and collected real tail latency data
* **Compared P95/P99 latency** across single GPU, naive multi-GPU scaling, and llm-d orchestration
* **Tested multiple scenarios** - single-turn large prompts and multi-turn chat conversations
* **Measured the user experience impact** - how much faster are responses for your most frustrated users?
* **Built a recommendation** focusing on customer satisfaction improvements, not just technical metrics

== Prerequisites

This workshop assumes you already have:

* **Experience with vLLM** - You understand how to deploy models and basic inference concepts
* **LLM inference knowledge** - Familiar with metrics like TTFT, ITL, throughput, and KV cache
* **Kubernetes/OpenShift skills** - Can work with kubectl/oc CLI, understand pods and services
* **KServe familiarity** - Know what InferenceServices and ServingRuntimes are
* **Grafana basics** - Can read dashboards and understand time-series metrics

**What we WON'T cover**:

* Transformer architecture fundamentals
* How vLLM works internally
* Kubernetes basics
* Introduction to LLM serving

This workshop focuses on **distributed inference optimization**, not foundational concepts.

== Your ParasolCloud scenario

Throughout the workshop, you'll work on a realistic business problem:

**The Challenge**: ParasolCloud's AI-powered customer service platform needs to scale significantly for a major client expansion launching in 90 days.

**Current State**:
* Single GPU vLLM deployment
* Limited request capacity
* Customer service chatbot with system prompt shared across queries

**Your Task**:
* Evaluate scaling options
* Benchmark naive scaling (more GPUs with round-robin load balancing)
* Test llm-d with intelligent cache-aware routing
* Provide recommendation with quantified ROI

**Success Criteria**:
* Achieve required throughput for client expansion
* Minimize GPU investment
* Improve latency for better customer experience
* Deploy within 90-day timeline

== Common questions before you start

**"I've never used llm-d before. Will I be able to follow along?"**

Yes! The workshop is designed for vLLM users who want to learn distributed inference. llm-d configuration is straightforward—you'll see it deploys in minutes.

**"What if I get stuck during exercises?"**

Ask for help! The instructor and other participants are resources. Each module also includes troubleshooting sections for common issues.

**"Can I use what I learn here in production?"**

Absolutely. llm-d is production-ready, and you'll leave with:
* Deployment patterns and configurations
* Benchmark methodologies
* Monitoring and observability setup
* Business case templates

**"What if my workload is different from the customer service example?"**

The principles apply broadly. Module 4 includes exercises testing different workload patterns (high prefix similarity, low similarity, mixed). You'll understand when llm-d provides high value vs. moderate value.

**"How technical is the ROI analysis?"**

We balance technical depth with business clarity. You'll calculate GPU cost savings, latency improvements, and efficiency gains—then practice translating these into messaging for executives, engineers, and operations teams.

**"What happens after the workshop?"**

Module 5 provides:
* Links to llm-d documentation and community resources
* Next steps for production adoption (staging deployment, gradual rollout)
* Advanced topics (prefill/decode disaggregation, MoE models)
* Contact information for ongoing support

== Workshop structure and timing

**Total duration**: 105 minutes (~2 hours)

* Module 1: Introduction & Core Concepts - 20 minutes
* Module 2: Deploy Model with vLLM & Benchmark - 25 minutes
* Module 3: Scale vLLM & Compare Performance - 25 minutes
* Module 4: Deploy llm-d & Benchmark - 35 minutes

**Pacing tips**:
* Modules 2-4 are hands-on. If you finish exercises early, help others or experiment with configurations.
* If you fall behind, the instructor will ensure key concepts are covered. You can complete exercises later.
* Screenshots and command outputs are provided for reference if time runs short.

== What success looks like

By the end of the workshop, you'll have:

✓ **Deployed three configurations** - single GPU, 4 GPU naive scaling, 4 GPU with llm-d
✓ **Generated real benchmark data** - P50/P95/P99 latency, cache hit rates
✓ **Demonstrated the critical insight** - More GPUs ≠ Better P95/P99
✓ **Measured P95/P99 latency improvements** with llm-d compared to naive scaling
✓ **Understanding when llm-d helps** - Shared prefixes and multi-turn conversations

**Most importantly**: You'll understand that llm-d's value is in **making your most frustrated customers happy** through tail latency reduction.

== Ready to begin?

Let's start by understanding your role and task. Navigate to the **Overview** section to learn about ParasolCloud's scaling challenge.

Then dive into **Module 1** to build the conceptual foundation for distributed inference optimization.

Welcome aboard, and let's unlock super-linear scaling together!
