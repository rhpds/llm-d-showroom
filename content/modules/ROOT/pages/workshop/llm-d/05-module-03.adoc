= Module 3: Scale vLLM & Compare Performance
:source-highlighter: rouge
:toc: macro
:toclevels: 2

You've measured your single-GPU tail latency baseline: P95 TTFT reaches 800-1200ms under load. ParasolCloud's customers are frustrated. The obvious solution: add more GPUs to reduce queuing.

In this module, you'll scale to 4 GPUs using naive horizontal scaling, run the same benchmarks, and discover a critical insight: **more GPUs increase capacity but don't fix tail latency**.

== Learning objectives
By the end of this module, you'll be able to:

* Scale vLLM deployment to multiple GPUs with round-robin load balancing
* Run benchmarks across both scenarios (single-turn and multi-turn)
* Demonstrate that naive scaling doesn't reduce P95/P99 latency
* Understand why isolated KV caches cause persistent tail latency problems

== Understanding naive scaling

Before deploying, understand what "naive scaling" means:

**Naive horizontal scaling**:
* Deploy multiple independent vLLM instances (1 per GPU)
* Use round-robin or random load balancing
* Each instance maintains its own isolated KV cache
* No coordination between instances

**What this achieves**:
* ✅ Increases capacity (more requests per second)
* ❌ **Does NOT reduce P95/P99 tail latency**

**Why tail latency doesn't improve**:
* **Cache misses persist**: Round-robin routing spreads requests randomly
* **Single-turn shared prefixes**: Every instance recomputes the 300-token system prompt
* **Multi-turn conversations**: Conversation turns land on different instances, causing cache misses
* **No coordination**: Instances don't know what other instances have cached

**The critical insight**: Adding GPUs gives you more *capacity* to handle traffic, but your most frustrated users (P95/P99) still experience high latency because cache misses cause redundant prefill computation.

== Exercise 1: Deploy vLLM with 4 replicas

You'll create a single InferenceService configured to scale to 4 replicas, with each replica running on its own GPU.

=== Clean up single-GPU deployment

First, remove your single-GPU deployment:

[source,bash]
----
# Delete previous InferenceService
oc delete inferenceservice llama-vllm-single

# Verify deletion
oc get inferenceservice
----

=== Deploy scaled vLLM InferenceService

Create an InferenceService with 4 replicas for naive horizontal scaling:

[source,bash]
----
cat > llama-vllm-scaled.yaml << 'EOF'
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llama-vllm-scaled
  annotations:
    openshift.io/display-name: llama-vllm-scaled
    serving.knative.openshift.io/enablePassthrough: 'true'
    sidecar.istio.io/inject: 'true'
    sidecar.istio.io/rewriteAppHTTPProbers: 'true'
  labels:
    opendatahub.io/dashboard: 'true'
spec:
  predictor:
    minReplicas: 4
    maxReplicas: 4
    model:
      modelFormat:
        name: vLLM
      runtime: vllm-runtime
      storageUri: hf://meta-llama/Meta-Llama-3.1-8B-Instruct
      resources:
        limits:
          cpu: '8'
          memory: 24Gi
          nvidia.com/gpu: '1'
        requests:
          cpu: '2'
          memory: 16Gi
          nvidia.com/gpu: '1'
    tolerations:
      - effect: NoSchedule
        key: nvidia.com/gpu
        operator: Exists
EOF
----

IMPORTANT: If you need to authenticate to Hugging Face (for gated models), add the `HF_TOKEN` environment variable as shown in Module 2.

Apply the InferenceService:

[source,bash]
----
# Deploy the scaled InferenceService
oc apply -f llama-vllm-scaled.yaml

# Watch deployment progress
watch oc get inferenceservice llama-vllm-scaled
----

Wait for the InferenceService to reach `Ready` state. This may take 10-15 minutes as all 4 replicas download and load the model.

Monitor progress:

[source,bash]
----
# Check all pods (should see 4 replicas)
oc get pods -l serving.kserve.io/inferenceservice=llama-vllm-scaled -w

# Check logs from all replicas
oc logs -l serving.kserve.io/inferenceservice=llama-vllm-scaled --all-containers -f
----

=== Verify the deployment

Check that all 4 replicas are running:

[source,bash]
----
# Verify 4 pods are running
oc get pods -l serving.kserve.io/inferenceservice=llama-vllm-scaled

# Get the inference URL
INFERENCE_URL=$(oc get inferenceservice llama-vllm-scaled -o jsonpath='{.status.url}')
echo "Inference URL: ${INFERENCE_URL}"

# Test the endpoint
curl -k ${INFERENCE_URL}/v1/models
----

The endpoint should respond with model information. With 4 replicas, Knative will automatically load balance requests in a round-robin fashion across all instances.

=== Understanding the naive load balancing

With this deployment:

* **Knative automatically load balances** requests across all 4 replicas
* **No session affinity**: Each request can go to any replica (round-robin)
* **Isolated KV caches**: Each replica maintains its own separate cache
* **No cache coordination**: Replicas don't share cache information

This is "naive scaling" - we get 4x capacity, but tail latency won't improve because:

1. Similar requests (with shared system prompts) land on different replicas → cache misses
2. Multi-turn conversations spread across replicas → cache misses on every turn
3. Round-robin routing ignores which replica has relevant cached prefixes

Let's benchmark to prove this.

== Exercise 2: Run scaled benchmarks

Now run the same benchmarks you performed in Module 2, testing both scenarios to measure tail latency with 4 GPUs.

**Critical question**: Does adding 4x GPUs reduce P95/P99 latency?

=== Update Grafana to monitor all replicas

Before benchmarking, update your ServiceMonitor to capture metrics from all replicas:

[source,bash]
----
cat > vllm-scaled-servicemonitor.yaml << 'EOF'
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: vllm-scaled-metrics
spec:
  selector:
    matchLabels:
      serving.kserve.io/inferenceservice: llama-vllm-scaled
  endpoints:
    - port: http
      path: /metrics
      interval: 30s
EOF
----

[source,bash]
----
oc apply -f vllm-scaled-servicemonitor.yaml
----

Create or update your Grafana dashboard to show aggregate metrics across all replicas, especially cache hit rates.

=== Scenario 1: Single-turn with large shared prefixes

Test whether adding GPUs reduces tail latency for requests with shared system prompts:

[source,bash]
----
# Get InferenceService URL
INFERENCE_URL=$(oc get inferenceservice llama-vllm-scaled -o jsonpath='{.status.url}')

# Run Scenario 1 - High load
guidellm \
  --target ${INFERENCE_URL}/v1/completions \
  --model meta-llama/Meta-Llama-3.1-8B-Instruct \
  --data single-turn-prompts.txt \
  --rate 20 \
  --max-requests 200 \
  --output-format json \
  --output-file scaled-4gpu-scenario1-high.json
----

**Expected P95/P99 latency** (compared to single GPU):
* **Single GPU P95 TTFT**: ~900ms
* **4 GPUs P95 TTFT**: ~850-950ms (**minimal improvement!**)
* **Capacity increased**: Yes (4x throughput)
* **Tail latency reduced**: ❌ **No significant improvement**

**Why?** Round-robin routing spreads requests across 4 replicas. Every replica recomputes the 300-token system prompt because cache misses occur on every request.

=== Scenario 2: Multi-turn chat conversations

Test whether adding GPUs improves multi-turn conversation latency:

[source,bash]
----
# Get InferenceService URL
INFERENCE_URL=$(oc get inferenceservice llama-vllm-scaled -o jsonpath='{.status.url}')

# Run Scenario 2 - Multi-turn chat
guidellm \
  --target ${INFERENCE_URL}/v1/completions \
  --model meta-llama/Meta-Llama-3.1-8B-Instruct \
  --data multi-turn-prompts.txt \
  --rate 5 \
  --max-requests 50 \
  --output-format json \
  --output-file scaled-4gpu-scenario2-multiturn.json
----

**Expected behavior**:
* Conversation turns randomly distributed across 4 replicas
* Each turn lands on a different replica (cache miss)
* **P95/P99 latency remains high** despite having 4 GPUs
* Growing context recomputed on every turn

**The problem**: With 4 replicas and round-robin routing, only 25% chance a follow-up lands on the same replica. 75% of conversation turns experience cache misses.

== Exercise 3: Analyze tail latency results

Compare your single-GPU and 4-GPU P95/P99 latency results.

=== Create tail latency comparison

[source,bash]
----
cat > tail-latency-comparison.txt << 'EOF'
=== Tail Latency Comparison: 1 GPU vs. 4 GPUs ===

Scenario 1: Single-turn with Large Shared Prefixes
---------------------------------------------------
Single GPU (High Load):
- P50 TTFT: ~400ms
- P95 TTFT: ~900ms
- P99 TTFT: ~1200ms
- Cache hit rate: ~10%

4 GPUs (High Load, Round-Robin):
- P50 TTFT: ~350ms (slight improvement)
- P95 TTFT: ~850ms (**barely improved**)
- P99 TTFT: ~1150ms (**barely improved**)
- Cache hit rate: ~8% (worse due to cache fragmentation!)

**Result**: Adding 4x GPUs does NOT significantly reduce tail latency.
**Your most frustrated users**: Still waiting 850-1150ms.

Scenario 2: Multi-turn Chat Conversations
------------------------------------------
Single GPU:
- P95 TTFT (Turn 1): ~600ms
- P95 TTFT (Turn 2): ~800ms
- P95 TTFT (Turn 3): ~900ms
- Cache behavior: Sequential turns on same GPU, some cache benefit

4 GPUs (Round-Robin):
- P95 TTFT (Turn 1): ~550ms
- P95 TTFT (Turn 2): ~850ms (**WORSE than single GPU!**)
- P95 TTFT (Turn 3): ~950ms (**WORSE than single GPU!**)
- Cache behavior: Turns land on different GPUs = cache misses

**Result**: Multi-turn conversations actually experience WORSE latency
with naive scaling because each turn hits a different instance.

The Fundamental Problem
-----------------------
**Capacity vs. Latency**:
- Capacity (throughput): 4 GPUs = 4x improvement ✅
- Tail latency (P95/P99): 4 GPUs = minimal or NO improvement ❌

**Why tail latency doesn't improve**:
- Every request recomputes 300-token system prompt (cache miss)
- Multi-turn conversations spread across replicas (no cache reuse)
- Round-robin routing ignores cache state (random placement)
- 4 isolated caches = 4x wasted prefill computation

**ParasolCloud's Challenge**:
- Customer complaints about slow responses continue
- P95 latency still 850-950ms (target: <300ms)
- Adding more GPUs won't solve the tail latency problem
- Need intelligent cache-aware routing to reduce P95/P99
EOF
----

=== Visualize the tail latency problem

The key problem is cache isolation causing high P95/P99 latency:

[source,text]
----
Scenario 1: Single-turn requests (round-robin routing)

Request 1: "System prompt... User asks: reset password?"
  → Routes to Replica 1 (TTFT: 400ms)
  → Computes full 350 tokens
  → Caches in Replica 1

Request 2: "System prompt... User asks: business hours?"
  → Routes to Replica 2 (TTFT: 900ms - P95!)
  → Cache miss! Recomputes 300-token system prompt
  → Hits queue behind other requests
  → **High P95 latency due to redundant prefill + queuing**

Request 3: "System prompt... User asks: track order?"
  → Routes to Replica 3 (TTFT: 850ms - P95!)
  → Cache miss! Recomputes system prompt
  → **Another frustrated user**

Scenario 2: Multi-turn conversation (round-robin routing)

Turn 1: "System prompt... Customer: I have a problem."
  → Routes to Replica 1 (TTFT: 500ms)
  → Computes full context

Turn 2: "System prompt... Customer: I have a problem. Agent: ... Customer: Follow-up?"
  → Routes to Replica 3 (TTFT: 950ms - P95!)
  → Cache miss! Recomputes entire conversation history
  → **Multi-turn latency spike**

Turn 3: Growing context...
  → Routes to Replica 2 (TTFT: 1100ms - P99!)
  → Cache miss! Recomputes everything again
  → **Your most frustrated user**
----

=== The tail latency insight

[source,bash]
----
cat > tail-latency-insight.txt << 'EOF'
=== Why More GPUs Don't Fix Tail Latency ===

The Capacity vs. Latency Distinction:
--------------------------------------
**Capacity (throughput)**: How many requests per second
- More GPUs = More capacity ✅
- 4 GPUs = 4x throughput (200 req/s)

**Tail Latency (P95/P99)**: Experience of your most frustrated users
- More GPUs ≠ Better P95/P99 ❌
- 4 GPUs = Same or worse P95/P99 latency

Why Naive Scaling Fails for Tail Latency:
------------------------------------------
1. **Cache fragmentation**: 4 isolated caches = 4x cache misses
2. **Random routing**: Unlucky users hit busy replicas or cache misses
3. **Redundant prefill**: Every replica recomputes shared prefixes
4. **Multi-turn disasters**: Conversations spread across replicas

ParasolCloud's Reality:
-----------------------
- ✅ Can handle 4x more traffic (capacity improved)
- ❌ Customers still complain about slow responses (P95 unchanged)
- ❌ Most frustrated users see no improvement
- ❌ Multi-turn conversations might be WORSE

The Solution Needed:
--------------------
**Intelligent cache-aware routing** that:
- Routes similar requests to same replica (cache hits)
- Routes conversation turns to same replica (context reuse)
- Eliminates redundant prefill computation
- **Directly reduces P95/P99 latency**
EOF
----

== Exercise 4: Observe cache behavior in Grafana

While benchmarks run, monitor vLLM metrics:

1. Open Grafana dashboard
2. Run a high-load benchmark
3. Observe:
   * **GPU cache usage per replica**: Each maintains its own cache
   * **Cache hit rate**: Very low (~5-10% at best)
   * **Compute utilization**: High prefill compute on all replicas
   * **Uneven load**: Some replicas busier than others

Take screenshots showing:
* Cache hit rates across 4 replicas
* GPU utilization per replica
* Request distribution

These visualizations demonstrate the problem llm-d solves.

== Module 3 summary

**What you accomplished:**
* Scaled vLLM deployment from 1 to 4 GPUs with round-robin load balancing
* Ran benchmarks for both scenarios: single-turn shared prefixes and multi-turn conversations
* Demonstrated the critical insight: **More GPUs ≠ Better P95/P99 latency**
* Measured that 4 GPUs provide minimal or no tail latency improvement

**Key takeaways:**
* **Capacity vs. Latency**: 4 GPUs increase throughput (capacity) but don't reduce tail latency
* **P95/P99 barely improves**: Single GPU ~900ms → 4 GPUs ~850ms (only 50ms improvement!)
* **Multi-turn gets worse**: Conversation turns spread across instances cause more cache misses
* **Critical insight**: Round-robin routing + isolated caches = persistent tail latency problems

**The fundamental problem:**
* 4 vLLM replicas = 4 isolated KV caches
* Round-robin routing spreads requests randomly (ignores cache state)
* System prompt (300 tokens) recomputed on every request (cache misses)
* Multi-turn conversations recompute entire history on each turn
* **Your most frustrated users (P95/P99) see no improvement**

**ParasolCloud's reality:**
* ✅ Capacity increased: 4x throughput (200 req/s)
* ❌ Customer complaints continue: P95 latency still ~850-950ms
* ❌ Target P95 latency <300ms: Not achieved
* ❌ Most frustrated users: Still experiencing slow responses

**The critical distinction:**
* **Scaling for capacity**: Add more GPUs → More requests/second ✅
* **Scaling for latency**: Need intelligent routing → Faster responses for worst-case users ❌

**The question:**
Can intelligent cache-aware routing dramatically reduce P95/P99 latency without adding more hardware?

**Next steps:**
Module 4 will deploy llm-d with Precise Prefix Cache Aware Routing. You'll configure llm-d to orchestrate your 4 vLLM replicas intelligently, routing requests to maximize cache hits and directly reduce P95/P99 latency. The goal: Deliver <300ms P95 TTFT to dramatically improve the experience for your most frustrated customers.
