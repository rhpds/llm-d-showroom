= Module 3: Scale vLLM & Compare Performance
:source-highlighter: rouge
:toc: macro
:toclevels: 2

You've measured your single-GPU tail latency baseline: P95 TTFT reaches 800-1200ms under load. ParasolCloud's customers are frustrated. The obvious solution: add more GPUs to reduce queuing.

In this module, you'll scale to 4 GPUs using naive horizontal scaling, run the same benchmarks, and discover a critical insight: **more GPUs increase capacity but don't fix tail latency**.

== Learning objectives
By the end of this module, you'll be able to:

* Scale vLLM deployment to multiple GPUs with round-robin load balancing
* Run benchmarks across both scenarios (single-turn and multi-turn)
* Demonstrate that naive scaling doesn't reduce P95/P99 latency
* Understand why isolated KV caches cause persistent tail latency problems

== Understanding naive scaling

Before deploying, understand what "naive scaling" means:

**Naive horizontal scaling**:

* Deploy multiple independent vLLM instances (1 per GPU)
* Use round-robin or random load balancing
* Each instance maintains its own isolated KV cache
* No coordination between instances

**What this achieves**:

* ✅ Increases capacity (more requests per second)
* ❌ **Does NOT reduce P95/P99 tail latency**

**Why tail latency doesn't improve**:

* **Cache misses persist**: Round-robin routing spreads requests randomly
* **Single-turn shared prefixes**: Every instance recomputes the 300-token system prompt
* **Multi-turn conversations**: Conversation turns land on different instances, causing cache misses
* **No coordination**: Instances don't know what other instances have cached

**The critical insight**: Adding GPUs gives you more *capacity* to handle traffic, but your most frustrated users (P95/P99) still experience high latency because cache misses cause redundant prefill computation.

== Exercise 1: Deploy vLLM with 4 replicas

You'll create a single InferenceService configured to scale to 4 replicas, with each replica running on its own GPU.

=== Clean up single-GPU deployment

First, remove your single-GPU deployment:

[.console-input]
[source,bash]
----
# Delete previous InferenceService
oc delete inferenceservice llama-vllm-single

# Verify deletion
oc get inferenceservice
----

=== Deploy scaled vLLM InferenceService

Create an InferenceService with 4 replicas for naive horizontal scaling:

[.console-input]
[source,bash]
----
cat > llama-vllm-scaled.yaml << 'EOF'
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llama-vllm-scaled
  annotations:
    serving.kserve.io/deploymentMode: RawDeployment
  labels:
    networking.kserve.io/visibility: exposed
spec:
  predictor:
    automountServiceAccountToken: false
    maxReplicas: 4
    minReplicas: 4
    model:
      modelFormat:
        name: vLLM
      name: ''
      resources:
        limits:
          cpu: '8'
          memory: 16Gi
          nvidia.com/gpu: '1'
        requests:
          cpu: '2'
          memory: 8Gi
          nvidia.com/gpu: '1'
      runtime: rhaiis-cuda
      storageUri: 'oci://registry.redhat.io/rhelai1/modelcar-llama-3-1-8b-instruct-fp8-dynamic:1.5'
      args:
        - --max-model-len=65536
EOF
----

IMPORTANT: If you need to authenticate to Hugging Face (for gated models), add the `HF_TOKEN` environment variable as shown in Module 2.

Apply the InferenceService:

[.console-input]
[source,bash]
----
# Deploy the scaled InferenceService
oc apply -f llama-vllm-scaled.yaml

# Watch deployment progress
watch oc get inferenceservice llama-vllm-scaled
----

Wait for the InferenceService to reach `Ready` state. This may take 10-15 minutes as all 4 replicas download and load the model.

Monitor progress:

[.console-input]
[source,bash]
----
# Check all pods (should see 4 replicas)
oc get pods -l serving.kserve.io/inferenceservice=llama-vllm-scaled -w
----

And check logs to see when each replica is ready:

[.console-input]
[source,bash]
----
# Check logs from all replicas
oc logs -l serving.kserve.io/inferenceservice=llama-vllm-scaled --all-containers -f
----

=== Verify the deployment

Check that all 4 replicas are running:

[.console-input]
[source,bash]
----
# Verify 4 pods are running
oc get pods -l serving.kserve.io/inferenceservice=llama-vllm-scaled

# Get the inference URL
INFERENCE_URL=$(oc get inferenceservice llama-vllm-scaled -o jsonpath='{.status.url}')
echo "Inference URL: ${INFERENCE_URL}"

# Test the endpoint
curl -k ${INFERENCE_URL}/v1/models | jq
----

The endpoint should respond with model information. 

Let's send one simple inference request before we move forward:

[.console-input]
[source,bash]
----
# Send a test request to the model
curl ${INFERENCE_URL}/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama-vllm-scaled",
    "prompt": "San Francisco is a",
    "max_tokens": 50,
    "temperature": 0.7
  }' | jq .
----

With 4 replicas, OpenShift will automatically load balance requests in a round-robin fashion across all instances.

=== Understanding the naive load balancing

With this deployment:

* **OpenShift automatically load balances** requests across all 4 replicas
* **No session affinity**: Each request can go to any replica (round-robin)
* **Isolated KV caches**: Each replica maintains its own separate cache
* **No cache coordination**: Replicas don't share cache information

This is "naive scaling" - we get 4x capacity, but tail latency won't improve because:

1. Similar requests (with shared system prompts) land on different replicas → cache misses
2. Multi-turn conversations spread across replicas → cache misses on every turn
3. Round-robin routing ignores which replica has relevant cached prefixes

Let's benchmark to prove this.

== Exercise 2: Run scaled benchmarks

Now run the same benchmarks you performed in Module 2, testing both scenarios to measure tail latency with 4 GPUs.

**Critical question**: Does adding 4x GPUs reduce P95/P99 latency?

=== Update Grafana to monitor all replicas

Before benchmarking, check your Grafana dashboard if it shows aggregate metrics across all replicas, especially cache hit rates.

=== Run the benchmark

Run the same benchmark you performed in Module 2, now against the scaled deployment:

[.console-input]
[source,bash]
----
# Get InferenceService URL
INFERENCE_URL=$(oc get inferenceservice llama-vllm-scaled -o jsonpath='{.status.url}')
echo "INFERENCE_URL: ${INFERENCE_URL}"

# Run baseline benchmark with sweep profile
cat << EOF | oc apply -f-
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: guidellm-vllm-scaled-sweep-results
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  volumeMode: Filesystem
---
apiVersion: batch/v1
kind: Job
metadata:
  name: guidellm-vllm-scaled-sweep
spec:
  backoffLimit: 4
  template:
    spec:
      serviceAccountName: default
      restartPolicy: Never
      containers:
        - name: guidellm
          image: ghcr.io/vllm-project/guidellm:v0.4.0
          imagePullPolicy: IfNotPresent
          command:
            - /opt/app-root/bin/guidellm
          args:
            - benchmark
            - run
            - --target=${INFERENCE_URL}
            - --profile=sweep
            - --max-seconds=60
            - --processor=RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8-dynamic
            - --data=prompt_tokens=256,output_tokens=128
          volumeMounts:
            - name: home
              mountPath: /home/guidellm
            - name: results
              mountPath: /results
      volumes:
        - name: home
          emptyDir: {}
        - name: results
          persistentVolumeClaim:
            claimName: guidellm-vllm-scaled-sweep-results
EOF
----

Check the logs of the Job to monitor progress:

[.console-input]
[source,bash]
----
# Follow the Job logs
oc logs -f job/guidellm-vllm-scaled-sweep
----

=== Compare results

Compare your P95 TTFT results:

* **Single GPU (Module 2)**: Record your baseline P95 TTFT
* **4 GPUs (this module)**: Record your scaled P95 TTFT

**Critical observation**: Despite having 4x the GPUs, you should observe that P95/P99 latency shows minimal improvement. Capacity (throughput) increases, but tail latency does not significantly improve.

**Why?** Round-robin routing spreads requests across 4 replicas. Each replica maintains its own isolated KV cache, so cache misses occur frequently - especially for:

1. **Single-turn requests**: Each replica recomputes shared system prompts
2. **Multi-turn conversations**: Conversation turns land on different replicas, causing cache misses on every turn

**The problem**: With 4 replicas and round-robin routing, only 25% chance a follow-up lands on the same replica. 75% of conversation turns experience cache misses.

== Exercise 3: Analyze tail latency results

Compare your single-GPU and 4-GPU P95/P99 latency results.

=== Record your comparison

Create a comparison of your results:

[.console-input]
[source,text]
----
=== Tail Latency Comparison: 1 GPU vs. 4 GPUs ===

Single GPU (Module 2):
- P95 TTFT: _______ (your recorded value)

4 GPUs with Round-Robin (Module 3):
- P95 TTFT: _______ (your recorded value)

Improvement: _______ %
----

=== The fundamental problem

**Capacity vs. Latency**:

* Capacity (throughput): 4 GPUs = ~4x improvement ✅
* Tail latency (P95/P99): 4 GPUs = minimal or NO improvement ❌

**Why tail latency doesn't improve**:

* Every request may recompute shared prefixes (cache miss)
* Multi-turn conversations spread across replicas (no cache reuse)
* Round-robin routing ignores cache state (random placement)
* 4 isolated caches = wasted prefill computation

**ParasolCloud's Challenge**:

* Customer complaints about slow responses continue
* Adding more GPUs won't solve the tail latency problem
* Need intelligent cache-aware routing to reduce P95/P99

=== Visualize the tail latency problem

The key problem is cache isolation causing high P95/P99 latency:

[source,text]
----
Single-turn requests (round-robin routing)
------------------------------------------
Request 1: "System prompt... User asks: reset password?"
  → Routes to Replica 1
  → Computes full prefix, caches result

Request 2: "System prompt... User asks: business hours?"
  → Routes to Replica 2 (round-robin)
  → Cache miss! Recomputes entire system prompt
  → **Redundant prefill computation**

Request 3: "System prompt... User asks: track order?"
  → Routes to Replica 3
  → Cache miss! Recomputes system prompt again
  → **Another cache miss**

Multi-turn conversation (round-robin routing)
---------------------------------------------
Turn 1: "Customer: I have a problem."
  → Routes to Replica 1
  → Computes full context, caches result

Turn 2: "Customer: I have a problem. Agent: ... Customer: Follow-up?"
  → Routes to Replica 3 (round-robin)
  → Cache miss! Recomputes entire conversation history
  → **Multi-turn latency spike**

Turn 3: Growing context...
  → Routes to Replica 2
  → Cache miss! Recomputes everything again
  → **Your most frustrated user**
----

=== The tail latency insight

**Why More GPUs Don't Fix Tail Latency:**

**Capacity (throughput)**: How many requests per second
* More GPUs = More capacity ✅

**Tail Latency (P95/P99)**: Experience of your most frustrated users
* More GPUs ≠ Better P95/P99 ❌

**Why Naive Scaling Fails for Tail Latency:**

1. **Cache fragmentation**: 4 isolated caches = cache misses
2. **Random routing**: Requests hit different replicas randomly
3. **Redundant prefill**: Every replica recomputes shared prefixes
4. **Multi-turn disasters**: Conversations spread across replicas

**The Solution Needed:**

**Intelligent cache-aware routing** that:

* Routes similar requests to same replica (cache hits)
* Routes conversation turns to same replica (context reuse)
* Eliminates redundant prefill computation
* **Directly reduces P95/P99 latency**

== Exercise 4: Observe cache behavior in Grafana

While benchmarks run, monitor vLLM metrics:

1. Open Grafana dashboard
2. Run a high-load benchmark
3. Observe:
   * **GPU cache usage per replica**: Each maintains its own cache
   * **Cache hit rate**: Very low (~5-10% at best)
   * **Compute utilization**: High prefill compute on all replicas
   * **Uneven load**: Some replicas busier than others

Take screenshots showing:
* Cache hit rates across 4 replicas
* GPU utilization per replica
* Request distribution

These visualizations demonstrate the problem llm-d solves.

== Module 3 summary

**What you accomplished:**

* Scaled vLLM deployment from 1 to 4 GPUs with round-robin load balancing
* Ran benchmarks to compare tail latency with more GPUs
* Demonstrated the critical insight: **More GPUs ≠ Better P95/P99 latency**

**Key takeaways:**

* **Capacity vs. Latency**: 4 GPUs increase throughput (capacity) but don't reduce tail latency
* **P95/P99 barely improves**: Compare your results - you should see minimal improvement
* **Critical insight**: Round-robin routing + isolated caches = persistent tail latency problems

**The fundamental problem:**

* 4 vLLM replicas = 4 isolated KV caches
* Round-robin routing spreads requests randomly (ignores cache state)
* Shared prefixes recomputed on every replica (cache misses)
* Multi-turn conversations recompute entire history on each turn
* **Your most frustrated users (P95/P99) see no improvement**

**ParasolCloud's reality:**

* ✅ Capacity increased with more GPUs
* ❌ Customer complaints continue: P95 latency not significantly improved
* ❌ Most frustrated users: Still experiencing slow responses

**The critical distinction:**

* **Scaling for capacity**: Add more GPUs → More requests/second ✅
* **Scaling for latency**: Need intelligent routing → Faster responses for worst-case users ❌

**The question:**
Can intelligent cache-aware routing dramatically reduce P95/P99 latency without adding more hardware?

**Next steps:**
Module 4 will deploy llm-d with Precise Prefix Cache Aware Routing. You'll configure llm-d to orchestrate your 4 vLLM replicas intelligently, routing requests to maximize cache hits and directly reduce P95/P99 latency.
