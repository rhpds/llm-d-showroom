= Module 3: Scale vLLM & Compare Performance
:source-highlighter: rouge
:toc: macro
:toclevels: 2

You've measured your single-GPU tail latency baseline: P95 TTFT reaches almost 5000ms under load.
ParasolCloud's customers are frustrated. The obvious solution: add more GPUs to reduce queuing.

In this module, you'll scale to 4 GPUs using naive horizontal scaling,
run the same benchmark, and discover a critical insight:
**more GPUs increase capacity but they can't always fix tail latency**.

== Learning objectives

By the end of this module, you'll be able to:

* Scale vLLM deployment to multiple GPUs with round-robin load balancing
* Run our custom multi-turn benchmark again, using the dashboard controls to compare and contrast
* Demonstrate that naive scaling doesn't reduce the worst of our latency
* Understand why isolated KV caches cause persistent tail latency problems

== Understanding naive scaling

Before deploying, understand what "naive scaling" means:

**Naive horizontal scaling**:

* Deploy multiple independent vLLM instances (1 per GPU)
* Use round-robin or random load balancing
* Each instance maintains its own isolated KV cache
* No coordination between instances

**What this achieves**:

* ✅ Increases capacity (more requests per second)
* ❌ **Does NOT guarantee reduction in P95/P99 tail latency**

**Why tail latency doesn't improve**:

* **Cache misses persist**: Round-robin routing spreads requests randomly
* **Multi-turn conversations**: Conversation turns land on different instances, causing cache misses
* **No coordination**: Instances don't know what other instances have cached

**The critical insight**: Adding GPUs gives you more *capacity* to handle traffic,
but your most frustrated users (P95/P99) still experience high latency because cache
misses cause redundant prefill computation, negating one of vLLM's greatest benefits.

== Exercise 1: Deploy vLLM with 4 replicas

You'll create a single InferenceService configured to scale to 4 replicas,
with each replica running on its own GPU.

=== Clean up single-GPU deployment

First, remove your single-GPU deployment:

[.console-input]
[source,bash]
----
# Delete previous InferenceService
oc delete inferenceservice llama-vllm-single
----

Expected output:

[.console-output]
[source,bash]
----
inferenceservice.serving.kserve.io "llama-vllm-single" deleted
----

=== Deploy scaled vLLM InferenceService

Create an InferenceService with 4 replicas for naive horizontal scaling:

[.console-input]
[source,bash]
----
cat << 'EOF' | oc apply -f-
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llama-vllm-scaled
  annotations:
    openshift.io/display-name: Llama 3.1 8B FP8 Scaled
    security.opendatahub.io/enable-auth: "false"
    serving.kserve.io/deploymentMode: RawDeployment
    opendatahub.io/hardware-profile-name: accelerated-profile
    opendatahub.io/hardware-profile-namespace: redhat-ods-applications
  labels:
    networking.kserve.io/visibility: exposed
spec:
  predictor:
    automountServiceAccountToken: false
    maxReplicas: 4
    minReplicas: 4
    deploymentStrategy:
      type: Recreate
    model:
      modelFormat:
        name: vLLM
      name: ''
      resources:
        limits:
          cpu: '8'
          memory: 16Gi
          nvidia.com/gpu: '1'
        requests:
          cpu: '2'
          memory: 8Gi
          nvidia.com/gpu: '1'
      runtime: rhaiis-cuda
      storageUri: 'oci://registry.redhat.io/rhelai1/modelcar-llama-3-1-8b-instruct-fp8-dynamic:1.5'
      args:
        - --max-model-len=65536
EOF
----

Expected output:

[.console-output]
[source,bash]
----
inferenceservice.serving.kserve.io/llama-vllm-scaled created
----

=== Monitor deployment progress

[NOTE]
====
As before, you can follow along in OpenShift AI's dashboard if you like.
====

Watch the InferenceService status:

[.console-input]
[source,bash]
----
# Watch InferenceService status, use Ctrl+C to cancel when it shows ready
oc get inferenceservice llama-vllm-scaled -w
----

Example output:

[.console-output]
[source,bash]
----

----

The deployment will progress to `READY` being `True` after the model is loaded. You can press `Ctrl+C` to cancel once it does.
Again, the model and serving runtime should already have been downloaded to your node, so this won't take too long.
The models are being loaded into VRAM, along with CUDA graphs and preallocating KV cache space.

=== Verify the deployment

Check that all 4 replicas are running:

[source,bash]
----
# Verify 4 pods are running
oc get pods -l serving.kserve.io/inferenceservice=llama-vllm-scaled

# Get the inference URL
INFERENCE_URL=$(oc get inferenceservice llama-vllm-scaled -o jsonpath='{.status.url}')
echo "Inference URL: ${INFERENCE_URL}"

# Test the endpoint
curl -k ${INFERENCE_URL}/v1/models
----

The endpoint should respond with model information. With 4 replicas, Knative will automatically load balance requests in a round-robin fashion across all instances.

=== Understanding the naive load balancing

With this deployment:

* **Knative automatically load balances** requests across all 4 replicas
* **No session affinity**: Each request can go to any replica (round-robin)
* **Isolated KV caches**: Each replica maintains its own separate cache
* **No cache coordination**: Replicas don't share cache information

This is "naive scaling" - we get 4x capacity, but tail latency won't improve because:

1. Similar requests (with shared system prompts) land on different replicas → cache misses
2. Multi-turn conversations spread across replicas → cache misses on every turn
3. Round-robin routing ignores which replica has relevant cached prefixes

Let's benchmark to prove this.

== Exercise 2: Run scaled benchmarks

Now run the same benchmarks you performed in Module 2, testing both scenarios to measure tail latency with 4 GPUs.

**Critical question**: Does adding 4x GPUs reduce P95/P99 latency?

=== Update Grafana to monitor all replicas

Before benchmarking, update your ServiceMonitor to capture metrics from all replicas:

[source,bash]
----
cat > vllm-scaled-servicemonitor.yaml << 'EOF'
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: vllm-scaled-metrics
spec:
  selector:
    matchLabels:
      serving.kserve.io/inferenceservice: llama-vllm-scaled
  endpoints:
    - port: http
      path: /metrics
      interval: 30s
EOF
----

[source,bash]
----
oc apply -f vllm-scaled-servicemonitor.yaml
----

Create or update your Grafana dashboard to show aggregate metrics across all replicas, especially cache hit rates.

=== Run the benchmark

Run the same benchmark you performed in Module 2, now against the scaled deployment:

[source,bash]
----
# Get InferenceService URL
INFERENCE_URL=$(oc get inferenceservice llama-vllm-scaled -o jsonpath='{.status.url}')
echo "INFERENCE_URL: ${INFERENCE_URL}"

# Run benchmark with sweep profile
guidellm benchmark \
  --target "${INFERENCE_URL}" \
  --profile sweep \
  --max-seconds 60 \
  --processor meta-llama/Meta-Llama-3.1-8B-Instruct \
  --data "prompt_tokens=256,output_tokens=128"
----

=== Compare results

Compare your P95 TTFT results:

* **Single GPU (Module 2)**: Record your baseline P95 TTFT
* **4 GPUs (this module)**: Record your scaled P95 TTFT

**Critical observation**: Despite having 4x the GPUs, you should observe that P95/P99 latency shows minimal improvement. Capacity (throughput) increases, but tail latency does not significantly improve.

**Why?** Round-robin routing spreads requests across 4 replicas. Each replica maintains its own isolated KV cache, so cache misses occur frequently - especially for:

1. **Single-turn requests**: Each replica recomputes shared system prompts
2. **Multi-turn conversations**: Conversation turns land on different replicas, causing cache misses on every turn

**The problem**: With 4 replicas and round-robin routing, only 25% chance a follow-up lands on the same replica. 75% of conversation turns experience cache misses.

== Exercise 3: Analyze tail latency results

Compare your single-GPU and 4-GPU P95/P99 latency results.

=== Record your comparison

Create a comparison of your results:

[source,text]
----
=== Tail Latency Comparison: 1 GPU vs. 4 GPUs ===

Single GPU (Module 2):
- P95 TTFT: _______ (your recorded value)

4 GPUs with Round-Robin (Module 3):
- P95 TTFT: _______ (your recorded value)

Improvement: _______ %
----

=== The fundamental problem

**Capacity vs. Latency**:
* Capacity (throughput): 4 GPUs = ~4x improvement ✅
* Tail latency (P95/P99): 4 GPUs = minimal or NO improvement ❌

**Why tail latency doesn't improve**:
* Every request may recompute shared prefixes (cache miss)
* Multi-turn conversations spread across replicas (no cache reuse)
* Round-robin routing ignores cache state (random placement)
* 4 isolated caches = wasted prefill computation

**ParasolCloud's Challenge**:
* Customer complaints about slow responses continue
* Adding more GPUs won't solve the tail latency problem
* Need intelligent cache-aware routing to reduce P95/P99

=== Visualize the tail latency problem

The key problem is cache isolation causing high P95/P99 latency:

[source,text]
----
Single-turn requests (round-robin routing)
------------------------------------------
Request 1: "System prompt... User asks: reset password?"
  → Routes to Replica 1
  → Computes full prefix, caches result

Request 2: "System prompt... User asks: business hours?"
  → Routes to Replica 2 (round-robin)
  → Cache miss! Recomputes entire system prompt
  → **Redundant prefill computation**

Request 3: "System prompt... User asks: track order?"
  → Routes to Replica 3
  → Cache miss! Recomputes system prompt again
  → **Another cache miss**

Multi-turn conversation (round-robin routing)
---------------------------------------------
Turn 1: "Customer: I have a problem."
  → Routes to Replica 1
  → Computes full context, caches result

Turn 2: "Customer: I have a problem. Agent: ... Customer: Follow-up?"
  → Routes to Replica 3 (round-robin)
  → Cache miss! Recomputes entire conversation history
  → **Multi-turn latency spike**

Turn 3: Growing context...
  → Routes to Replica 2
  → Cache miss! Recomputes everything again
  → **Your most frustrated user**
----

=== The tail latency insight

**Why More GPUs Don't Fix Tail Latency:**

**Capacity (throughput)**: How many requests per second
* More GPUs = More capacity ✅

**Tail Latency (P95/P99)**: Experience of your most frustrated users
* More GPUs ≠ Better P95/P99 ❌

**Why Naive Scaling Fails for Tail Latency:**

1. **Cache fragmentation**: 4 isolated caches = cache misses
2. **Random routing**: Requests hit different replicas randomly
3. **Redundant prefill**: Every replica recomputes shared prefixes
4. **Multi-turn disasters**: Conversations spread across replicas

**The Solution Needed:**

**Intelligent cache-aware routing** that:
* Routes similar requests to same replica (cache hits)
* Routes conversation turns to same replica (context reuse)
* Eliminates redundant prefill computation
* **Directly reduces P95/P99 latency**

== Exercise 4: Observe cache behavior in Grafana

While benchmarks run, monitor vLLM metrics:

1. Open Grafana dashboard
2. Run a high-load benchmark
3. Observe:
   * **GPU cache usage per replica**: Each maintains its own cache
   * **Cache hit rate**: Very low (~5-10% at best)
   * **Compute utilization**: High prefill compute on all replicas
   * **Uneven load**: Some replicas busier than others

Take screenshots showing:
* Cache hit rates across 4 replicas
* GPU utilization per replica
* Request distribution

These visualizations demonstrate the problem llm-d solves.

== Module 3 summary

**What you accomplished:**
* Scaled vLLM deployment from 1 to 4 GPUs with round-robin load balancing
* Ran benchmarks to compare tail latency with more GPUs
* Demonstrated the critical insight: **More GPUs ≠ Better P95/P99 latency**

**Key takeaways:**
* **Capacity vs. Latency**: 4 GPUs increase throughput (capacity) but don't reduce tail latency
* **P95/P99 barely improves**: Compare your results - you should see minimal improvement
* **Critical insight**: Round-robin routing + isolated caches = persistent tail latency problems

**The fundamental problem:**
* 4 vLLM replicas = 4 isolated KV caches
* Round-robin routing spreads requests randomly (ignores cache state)
* Shared prefixes recomputed on every replica (cache misses)
* Multi-turn conversations recompute entire history on each turn
* **Your most frustrated users (P95/P99) see no improvement**

**ParasolCloud's reality:**
* ✅ Capacity increased with more GPUs
* ❌ Customer complaints continue: P95 latency not significantly improved
* ❌ Most frustrated users: Still experiencing slow responses

**The critical distinction:**
* **Scaling for capacity**: Add more GPUs → More requests/second ✅
* **Scaling for latency**: Need intelligent routing → Faster responses for worst-case users ❌

**The question:**
Can intelligent cache-aware routing dramatically reduce P95/P99 latency without adding more hardware?

**Next steps:**
Module 4 will deploy llm-d with Precise Prefix Cache Aware Routing. You'll configure llm-d to orchestrate your 4 vLLM replicas intelligently, routing requests to maximize cache hits and directly reduce P95/P99 latency.
