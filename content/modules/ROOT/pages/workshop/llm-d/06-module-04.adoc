= Module 4: Deploy llm-d & Benchmark
:source-highlighter: rouge
:toc: macro
:toclevels: 2

You've proven that 4 GPUs with naive scaling don't reduce P95/P99 tail latency—your most frustrated users still wait 850-950ms. Now it's time to deploy llm-d with Precise Prefix Cache Aware Routing to dramatically reduce tail latency through intelligent cache reuse.

In this module, you'll deploy llm-d to orchestrate your 4 vLLM instances with cache-aware routing, run the same benchmarks, and measure the P95/P99 improvements.

== Learning objectives
By the end of this module, you'll be able to:

* Deploy llm-d with Precise Prefix Cache Aware Routing configuration
* Configure llm-d to orchestrate multiple vLLM backend instances
* Set up llm-d Grafana dashboard to monitor cache hit rates
* Execute benchmarks across both scenarios and measure P95/P99 latency reductions
* Quantify customer experience improvements with real tail latency data

== Understanding llm-d deployment architecture

Before deploying, understand how llm-d integrates with your existing vLLM instances:

[source,text]
----
                    Client Requests
                           |
                           v
                   ┌───────────────┐
                   │  llm-d Pod    │
                   │  (Orchestrator)│
                   └───────┬───────┘
                           |
        ┌──────────────────┼──────────────────┐
        |                  |                  |
        v                  v                  v
  ┌──────────┐       ┌──────────┐       ┌──────────┐
  │ vLLM     │       │ vLLM     │       │ vLLM     │
  │ Instance │       │ Instance │       │ Instance │
  │ 1 (GPU1) │       │ 2 (GPU2) │  ...  │ 4 (GPU4) │
  └──────────┘       └──────────┘       └──────────┘

llm-d routing logic:
1. Receives request from client
2. Analyzes prompt prefix
3. Computes similarity to cached prefixes per backend
4. Routes to backend with highest cache hit probability
5. Returns response to client
----

**Key insight**: Your 4 vLLM instances keep running. llm-d sits in front as an intelligent reverse proxy.

== Exercise 1: Deploy llm-d with LLMInferenceService

=== Understanding LLMInferenceService

llm-d uses the **LLMInferenceService** custom resource to deploy an intelligent orchestration layer with precise-prefix-cache-aware routing. This is a Kubernetes-native approach that:

* Deploys multiple vLLM inference pool replicas
* Configures the llm-d router with intelligent scheduling plugins
* Enables **precise-prefix-cache-scorer** for cache-aware routing

=== Create the LLMInferenceService

Create the llm-d configuration with 4 replicas and precise-prefix-cache-aware routing:

[source,bash]
----
cat > llm-d-inference.yaml << 'EOF'
apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceService
metadata:
  name: llama-llm-d
  annotations:
    opendatahub.io/model-type: generative
    openshift.io/display-name: llama-llm-d
    security.opendatahub.io/enable-auth: 'false'
spec:
  replicas: 4
  model:
    uri: hf://meta-llama/Meta-Llama-3.1-8B-Instruct
    name: meta-llama/Meta-Llama-3.1-8B-Instruct
  router:
    scheduler:
      template:
        containers:
          - name: main
            env:
              - name: TOKENIZER_CACHE_DIR
                value: /tmp/tokenizer-cache
              - name: HF_HOME
                value: /tmp/tokenizer-cache
              - name: TRANSFORMERS_CACHE
                value: /tmp/tokenizer-cache
              - name: XDG_CACHE_HOME
                value: /tmp
            args:
              - --pool-group
              - inference.networking.x-k8s.io
              - '--pool-name'
              - '{{ ChildName .ObjectMeta.Name `-inference-pool` }}'
              - '--pool-namespace'
              - '{{ .ObjectMeta.Namespace }}'
              - '--zap-encoder'
              - json
              - '--grpc-port'
              - '9002'
              - '--grpc-health-port'
              - '9003'
              - '--secure-serving'
              - '--model-server-metrics-scheme'
              - https
              - '--config-text'
              - |
                apiVersion: inference.networking.x-k8s.io/v1alpha1
                kind: EndpointPickerConfig
                plugins:
                - type: single-profile-handler
                - type: queue-scorer
                - type: kv-cache-utilization-scorer
                - type: precise-prefix-cache-scorer
                schedulingProfiles:
                - name: default
                  plugins:
                  - pluginRef: queue-scorer
                    weight: 2
                  - pluginRef: kv-cache-utilization-scorer
                    weight: 2
                  - pluginRef: precise-prefix-cache-scorer
                    weight: 3
            volumeMounts:
              - name: tokenizer-cache
                mountPath: /tmp/tokenizer-cache
        volumes:
          - name: tokenizer-cache
            emptyDir: {}
    route: {}
    gateway: {}
  template:
    tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
    containers:
      - name: main
        env:
          - name: VLLM_ADDITIONAL_ARGS
            value: "--disable-uvicorn-access-log --max-model-len=4096 --gpu-memory-utilization=0.9"
        resources:
          limits:
            cpu: '1'
            memory: 8Gi
            nvidia.com/gpu: "1"
          requests:
            cpu: '1'
            memory: 8Gi
            nvidia.com/gpu: "1"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
            scheme: HTTPS
          initialDelaySeconds: 120
          periodSeconds: 30
          timeoutSeconds: 30
          failureThreshold: 5
EOF
----

**Key configuration elements**:

* **`replicas: 4`**: Deploys 4 vLLM inference pool instances
* **`precise-prefix-cache-scorer`**: The intelligent routing plugin (weight: 3)
* **`queue-scorer`**: Balances queue depth (weight: 2)
* **`kv-cache-utilization-scorer`**: Optimizes cache memory usage (weight: 2)
* **Plugin weights**: Higher weight = more influence on routing decisions

=== Apply the LLMInferenceService

[source,bash]
----
oc apply -f llm-d-inference.yaml
----

=== Monitor deployment progress

Watch the LLMInferenceService deployment:

[source,bash]
----
# Watch LLMInferenceService status
oc get llminferenceservice llama-llm-d -w

# Watch inference pool pods being created (4 replicas)
oc get pods -l serving.kserve.io/inferenceservice=llama-llm-d -w
----

The deployment will go through these states:
1. **Creating**: LLMInferenceService resource accepted
2. **InProgress**: Router and inference pool pods deploying, models downloading
3. **Ready**: All 4 inference pool instances ready, router active

This can take 10-15 minutes as each instance downloads the model.

**What gets deployed**:
* **1 Router pod**: The llm-d scheduler with precise-prefix-cache-aware routing
* **4 Inference pool pods**: vLLM instances (1 per GPU) managed by llm-d
* **Services and Routes**: Automatically created for external access

=== Verify llm-d is running

Once the LLMInferenceService shows `Ready`, get the inference URL:

[source,bash]
----
# Get the llm-d inference URL
LLMD_URL=$(oc get llminferenceservice llama-llm-d -o jsonpath='{.status.url}')
echo "llm-d URL: ${LLMD_URL}"

# Test with a simple request
curl -k ${LLMD_URL}/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Meta-Llama-3.1-8B-Instruct",
    "prompt": "San Francisco is a",
    "max_tokens": 20
  }'
----

Expected output: llm-d's router analyzes the prompt, routes to an inference pool instance based on cache state, and returns a completion.

=== Verify intelligent routing is active

Check the router logs to see precise-prefix-cache-scorer in action:

[source,bash]
----
# Get router pod name
ROUTER_POD=$(oc get pods -l app.kubernetes.io/component=router -o jsonpath='{.items[0].metadata.name}')

# View router logs showing routing decisions
oc logs $ROUTER_POD --tail=50

# You should see log lines indicating:
# - Prefix similarity analysis
# - Scoring plugin results (queue, kv-cache-utilization, precise-prefix-cache)
# - Backend selection based on cache awareness
----

=== Troubleshooting

**Issue**: LLMInferenceService stuck in "InProgress"
**Solution**:
[source,bash]
----
# Check LLMInferenceService status
oc describe llminferenceservice llama-llm-d

# Check router pod status
oc get pods -l app.kubernetes.io/component=router
oc describe pod -l app.kubernetes.io/component=router

# Check inference pool pods
oc get pods -l serving.kserve.io/inferenceservice=llama-llm-d
oc logs -l serving.kserve.io/inferenceservice=llama-llm-d --tail=100

# Common issues:
# - GPU quota: Verify 4 GPUs available
# - Image pull: Check image access and pull secrets
# - Model download: Check HuggingFace access (may need HF_TOKEN)
----

**Issue**: Model download timeout
**Solution**: If models are pre-cached on PVC, update the `uri`:
[source,yaml]
----
spec:
  model:
    uri: pvc://model-cache/llama-3.1-8b
----

== Exercise 2: Configure llm-d monitoring

Set up comprehensive monitoring for llm-d router and inference pool metrics.

=== Create ServiceMonitor for llm-d router

Monitor the router's scheduling decisions and cache hit rates:

[source,bash]
----
cat > llm-d-router-servicemonitor.yaml << 'EOF'
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: llm-d-router-metrics
spec:
  selector:
    matchLabels:
      app.kubernetes.io/component: router
  endpoints:
    - port: metrics
      path: /metrics
      interval: 30s
      scheme: https
      tlsConfig:
        insecureSkipVerify: true
EOF
----

Apply the ServiceMonitor:

[source,bash]
----
oc apply -f llm-d-router-servicemonitor.yaml
----

=== Create ServiceMonitor for inference pools

Monitor the vLLM inference pool instances:

[source,bash]
----
cat > llm-d-pool-servicemonitor.yaml << 'EOF'
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: llm-d-pool-metrics
spec:
  selector:
    matchLabels:
      serving.kserve.io/inferenceservice: llama-llm-d
  endpoints:
    - port: metrics
      path: /metrics
      interval: 30s
      scheme: https
      tlsConfig:
        insecureSkipVerify: true
EOF
----

Apply the ServiceMonitor:

[source,bash]
----
oc apply -f llm-d-pool-servicemonitor.yaml
----

=== Import llm-d Grafana dashboard

Import the llm-d dashboard to visualize:
* Request routing decisions
* Cache hit rates per backend
* Latency per backend
* Throughput and load distribution

1. Open Grafana
2. Navigate to **Dashboards → Import**
3. Upload `llm-d-dashboard.json` (provided in workshop materials)
4. Select Prometheus data source
5. Click **Import**

Key panels in llm-d dashboard:
* **Cache hit rate**: Percentage of requests benefiting from KV cache
* **Routing distribution**: Which backend received each request
* **Backend utilization**: Load per vLLM instance
* **Prefix similarity**: How similar incoming requests are
* **Throughput comparison**: Aggregate throughput across all backends

=== Verify metrics are flowing

[source,bash]
----
# Get router pod name
ROUTER_POD=$(oc get pods -l app.kubernetes.io/component=router -o jsonpath='{.items[0].metadata.name}')

# Port-forward to router metrics endpoint
oc port-forward $ROUTER_POD 9090:9090 &

# Fetch router metrics
curl -k https://localhost:9090/metrics | grep -E '(scheduler|cache|queue)'

# Stop port-forward
pkill -f "port-forward"
----

Look for metrics like:
* `scheduler_endpoint_picker_scoring_duration_seconds`: Routing decision time
* `scheduler_queue_depth`: Queue length per inference pool
* Cache-related metrics from precise-prefix-cache-scorer
* `vllm:num_requests_running` and `vllm:gpu_cache_usage_perc` from inference pools

== Exercise 3: Run llm-d benchmarks

Now run the same two scenarios against llm-d to measure P95/P99 tail latency improvements from cache-aware routing.

**Goal**: Demonstrate that intelligent routing dramatically reduces tail latency for your most frustrated users.

=== Warm up the system

Before benchmarking, send some requests to prime the caches:

[source,bash]
----
# Send warmup requests with shared prefixes
for i in {1..50}; do
  curl -k https://${LLMD_URL}/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
      "model": "meta-llama/Meta-Llama-3.1-8B-Instruct",
      "prompt": "You are a helpful customer service agent. User asks: How can I help you?",
      "max_tokens": 30
    }' &
done
wait
----

This primes the KV caches with the system prompt across llm-d's routing decisions.

=== Scenario 1: Single-turn with large shared prefixes

Test whether cache-aware routing reduces tail latency for shared system prompts:

[source,bash]
----
# Get llm-d URL
LLMD_URL=$(oc get route llm-d -o jsonpath='{.spec.host}')

# Run Scenario 1 - High load
guidellm \
  --target https://${LLMD_URL}/v1/completions \
  --model meta-llama/Meta-Llama-3.1-8B-Instruct \
  --data single-turn-prompts.txt \
  --rate 20 \
  --max-requests 200 \
  --output-format json \
  --output-file llmd-4gpu-scenario1-high.json
----

**Expected P95/P99 latency improvements**:
* **Naive 4 GPU P95 TTFT**: ~850ms
* **llm-d 4 GPU P95 TTFT**: ~400-500ms → **40-50% improvement!**
* **Naive 4 GPU P99 TTFT**: ~1150ms
* **llm-d 4 GPU P99 TTFT**: ~600-700ms → **45-50% improvement!**

**Why it works**:
* llm-d routes requests with shared prefixes to same backend
* Cache hits eliminate 300-token system prompt computation
* P95/P99 users benefit from cached prefill
* **Your most frustrated users now get much faster responses**

=== Scenario 2: Multi-turn chat conversations

Test whether cache-aware routing dramatically improves multi-turn conversation latency:

[source,bash]
----
# Run Scenario 2 - Multi-turn chat
guidellm \
  --target https://${LLMD_URL}/v1/completions \
  --model meta-llama/Meta-Llama-3.1-8B-Instruct \
  --data multi-turn-prompts.txt \
  --rate 5 \
  --max-requests 50 \
  --output-format json \
  --output-file llmd-4gpu-scenario2-multiturn.json
----

**Expected behavior**:
* Conversation turns routed to same backend (cache reuse!)
* Each turn benefits from cached context
* **P95/P99 latency dramatically reduced** compared to naive scaling

**Comparison with naive scaling**:
* **Naive Turn 2 P95**: ~850ms (cache miss, recomputed history)
* **llm-d Turn 2 P95**: ~350-450ms → **~50% improvement!**
* **Naive Turn 3 P95**: ~950ms
* **llm-d Turn 3 P95**: ~400-500ms → **~50% improvement!**

**The key difference**: llm-d routes follow-up turns to the same backend, ensuring cache hits. Naive scaling routes randomly, causing cache misses and forcing recomputation of growing conversation history.

== Exercise 4: Analyze llm-d performance gains

Compare all three configurations: single GPU, naive 4 GPU, and llm-d 4 GPU by analyzing your benchmark results.

== Exercise 5: Monitor cache effectiveness in Grafana

Watch llm-d's cache-aware routing directly improve tail latency in real-time:

1. Open Grafana llm-d dashboard
2. Run Scenario 1 high-load benchmark
3. Observe in real-time:
   * **Cache hit rate**: Should reach 60-80% for workload with shared prefixes
   * **Routing decisions**: Requests with similar prefixes consistently route to same backends
   * **Backend utilization**: Uneven but intentional (hot caches get more traffic)
   * **P95/P99 latency**: Significantly lower than naive scaling

**Key metrics to watch**:
* **Cache hit rate climbing**: Starts low, climbs to 60-80% as system warms up
* **P95 TTFT dropping**: From 850ms (naive) to 400-500ms (llm-d)
* **Routing affinity**: Similar prompts cluster on same backend

Compare this to the naive scaling dashboard from Module 3:
* **Naive**: All backends equally loaded, 5-10% cache hit rate, P95 TTFT ~850ms
* **llm-d**: Uneven but intentional distribution, 60-80% cache hit rate, P95 TTFT ~450ms

Take screenshots showing:
* Cache hit rate over time (climbing to 60-80%)
* P95/P99 latency comparison chart
* Routing distribution showing cache affinity

== Exercise 6: Test with different workload patterns

Understand when llm-d provides value by testing different prompt patterns.

=== Test 1: High prefix similarity (ParasolCloud's actual workload)

This is what you've been testing - shared system prompts.

Results: 70% cache hit rate, 53% throughput improvement.

=== Test 2: Low prefix similarity

Create a dataset with completely unique prompts:

[source,bash]
----
cat > unique-prompts.txt << 'EOF'
Write a poem about mountains
Explain quantum physics
What is the capital of Peru
Describe a sunset
How does photosynthesis work
Why is the sky blue
Tell me about ancient Rome
Explain machine learning
EOF
----

Run benchmark with unique prompts:

[source,bash]
----
guidellm \
  --target https://${LLMD_URL}/v1/completions \
  --model meta-llama/Meta-Llama-3.1-8B-Instruct \
  --data unique-prompts.txt \
  --rate 50 \
  --max-requests 200 \
  --output-format json \
  --output-file llmd-unique-prompts.json
----

Expected results:
* Cache hit rate: ~5-10% (minimal sharing)
* Throughput: ~180-200 req/s (similar to naive scaling)
* llm-d overhead: ~5% (minimal)

**Insight**: llm-d doesn't hurt performance for unique prompts, but doesn't help much either. Benefit depends on workload prefix similarity.

=== Test 3: Partial prefix sharing

Mix of shared and unique:

[source,bash]
----
cat > mixed-prompts.txt << 'EOF'
You are a helpful customer service agent. User asks: How do I reset my password?
Translate this to French: Hello world
You are a helpful customer service agent. User asks: What are your business hours?
Write a haiku about coding
You are a helpful customer service agent. User asks: How can I track my order?
EOF
----

Run benchmark:

[source,bash]
----
guidellm \
  --target https://${LLMD_URL}/v1/completions \
  --model meta-llama/Meta-Llama-3.1-8B-Instruct \
  --data mixed-prompts.txt \
  --rate 50 \
  --max-requests 200 \
  --output-format json \
  --output-file llmd-mixed-prompts.json
----

Expected results:
* Cache hit rate: ~30-40% (partial sharing)
* Throughput: ~210-230 req/s (moderate improvement)

**Insight**: Even partial prefix sharing provides measurable benefits.

== Module 4 summary

**What you accomplished:**
* Deployed llm-d orchestrator with Precise Prefix Cache Aware Routing
* Configured llm-d to intelligently manage 4 vLLM backend instances
* Set up comprehensive monitoring with Grafana to track cache hits and P95/P99 latency
* Ran benchmarks across both scenarios demonstrating **dramatic tail latency reductions**
* Proved that intelligent routing solves the problem naive scaling couldn't

**Key takeaways - The Tail Latency Breakthrough:**
* **P95 TTFT**: Improved from ~850ms (naive) to ~450ms (llm-d) = **~50% reduction!**
* **P99 TTFT**: Improved from ~1150ms (naive) to ~650ms (llm-d) = **~45% reduction!**
* **Multi-turn P95**: Improved from ~850ms (naive) to ~400ms (llm-d) = **~53% reduction!**
* **Cache hit rates**: Improved from 5-10% (naive) to 60-80% (llm-d)
* **Your most frustrated users**: Now get responses **2x faster**

**The critical insight:**
* Same hardware (4 GPUs)
* Same capacity (requests per second)
* But **dramatically different customer experience** for P95/P99 users
* **Cache-aware routing eliminates the tail latency problem**

**ParasolCloud's customer experience transformation:**
* **Before (naive 4 GPU)**: 5% of customers wait 850-1150ms → frustrated, file complaints
* **After (llm-d 4 GPU)**: 5% of customers wait 450-650ms → satisfied, positive experience
* **Impact**: Reduced customer complaints, improved satisfaction scores, better retention
* **Target achieved**: P95 TTFT now within 450-500ms range (target was <300ms, much closer!)

**Why llm-d works for tail latency:**
* **Analyzes prompt prefixes** and routes similar requests to same backend
* **Maximizes KV cache hits**: 60-80% of requests benefit from cached prefill
* **Eliminates redundant computation**: Shared 300-token prefixes cached, not recomputed
* **Routes multi-turn to same backend**: Conversation history stays cached
* **Directly attacks the P95/P99 problem**: Cache hits mean your worst-case users avoid queuing

**Scenario results summary:**
* **Scenario 1 (single-turn shared prefixes)**: 50% P95 TTFT reduction
* **Scenario 2 (multi-turn conversations)**: 53% P95 TTFT reduction for follow-ups
* **Benefit scales with prefix similarity**: More sharing = more improvement

**When llm-d provides value:**
* Workloads with shared prefixes (system prompts, common contexts)
* Multi-turn conversations requiring context reuse
* Any scenario where cache misses cause high P95/P99 latency
* When customer experience (not just capacity) matters

**Workshop complete!**

You've successfully demonstrated that intelligent cache-aware routing dramatically reduces tail latency for your most frustrated users. You now have the data and experience to advocate for llm-d deployment in production environments where P95/P99 latency matters.
