= Module 4: Deploy llm-d & Benchmark
:source-highlighter: rouge
:toc: macro
:toclevels: 2

You've proven that 4 GPUs with naive scaling don't significantly reduce P95/P99 tail latency—your most frustrated users still experience slow responses. Now it's time to deploy llm-d with Precise Prefix Cache Aware Routing to dramatically reduce tail latency through intelligent cache reuse.

In this module, you'll deploy llm-d to orchestrate your 4 vLLM instances with cache-aware routing, run the same benchmarks, and measure the P95/P99 improvements.

== Learning objectives
By the end of this module, you'll be able to:

* Deploy llm-d with Precise Prefix Cache Aware Routing configuration
* Configure llm-d to orchestrate multiple vLLM backend instances
* Set up llm-d Grafana dashboard to monitor cache hit rates
* Execute benchmarks across both scenarios and measure P95/P99 latency reductions
* Quantify customer experience improvements with real tail latency data

== Understanding llm-d deployment architecture

Before deploying, understand how llm-d integrates with your existing vLLM instances:

[source,text]
----
                    Client Requests
                           |
                           v
                   ┌───────────────┐
                   │  llm-d Pod    │
                   │  (Orchestrator)│
                   └───────┬───────┘
                           |
        ┌──────────────────┼──────────────────┐
        |                  |                  |
        v                  v                  v
  ┌──────────┐       ┌──────────┐       ┌──────────┐
  │ vLLM     │       │ vLLM     │       │ vLLM     │
  │ Instance │       │ Instance │       │ Instance │
  │ 1 (GPU1) │       │ 2 (GPU2) │  ...  │ 4 (GPU4) │
  └──────────┘       └──────────┘       └──────────┘

llm-d routing logic:
1. Receives request from client
2. Analyzes prompt prefix
3. Computes similarity to cached prefixes per backend
4. Routes to backend with highest cache hit probability
5. Returns response to client
----

**Key insight**: Your 4 vLLM instances keep running. llm-d sits in front as an intelligent reverse proxy.

== Exercise 1: Deploy llm-d with LLMInferenceService

=== Understanding LLMInferenceService

llm-d uses the **LLMInferenceService** custom resource to deploy an intelligent orchestration layer with precise-prefix-cache-aware routing. This is a Kubernetes-native approach that:

* Deploys multiple vLLM inference pool replicas
* Configures the llm-d router with intelligent scheduling plugins
* Enables **precise-prefix-cache-scorer** for cache-aware routing

=== Create HuggingFace token secret

The LLMInferenceService needs access to HuggingFace to download the model and tokenizer. Create a secret with your HuggingFace token:

[source,bash]
----
# Create secret with your HuggingFace token
oc create secret generic hf-secret \
  --from-literal=HF_TOKEN=your_hf_token_here
----

NOTE: If your workshop environment has pre-cached models, this step may be optional. Check with your instructor.

=== Create the LLMInferenceService

Create the llm-d configuration with 4 replicas and precise-prefix-cache-aware routing:

[source,bash]
----
cat > llm-d-inference.yaml << 'EOF'
apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceService
metadata:
  name: llama-llm-d
  annotations:
    opendatahub.io/model-type: generative
    openshift.io/display-name: llama-llm-d
    security.opendatahub.io/enable-auth: 'false'
    serving.kserve.io/storageSecretName: hf-secret
spec:
  replicas: 4
  model:
    uri: hf://meta-llama/Meta-Llama-3.1-8B-Instruct
    name: meta-llama/Meta-Llama-3.1-8B-Instruct
  router:
    scheduler:
      template:
        containers:
          - name: main
            env:
              - name: TOKENIZER_CACHE_DIR
                value: /tmp/tokenizer-cache
              - name: HF_HOME
                value: /tmp/tokenizer-cache
              - name: TRANSFORMERS_CACHE
                value: /tmp/tokenizer-cache
              - name: XDG_CACHE_HOME
                value: /tmp
            args:
              - --pool-group
              - inference.networking.x-k8s.io
              - '--pool-name'
              - '{{ ChildName .ObjectMeta.Name `-inference-pool` }}'
              - '--pool-namespace'
              - '{{ .ObjectMeta.Namespace }}'
              - '--zap-encoder'
              - json
              - '--grpc-port'
              - '9002'
              - '--grpc-health-port'
              - '9003'
              - '--secure-serving'
              - '--model-server-metrics-scheme'
              - https
              - '--config-text'
              - |
                apiVersion: inference.networking.x-k8s.io/v1alpha1
                kind: EndpointPickerConfig
                plugins:
                - type: single-profile-handler
                - type: queue-scorer
                - type: kv-cache-utilization-scorer
                - type: precise-prefix-cache-scorer
                schedulingProfiles:
                - name: default
                  plugins:
                  - pluginRef: queue-scorer
                    weight: 2
                  - pluginRef: kv-cache-utilization-scorer
                    weight: 2
                  - pluginRef: precise-prefix-cache-scorer
                    weight: 3
            volumeMounts:
              - name: tokenizer-cache
                mountPath: /tmp/tokenizer-cache
        volumes:
          - name: tokenizer-cache
            emptyDir: {}
    route: {}
    gateway: {}
  template:
    tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
    containers:
      - name: main
        env:
          - name: HF_TOKEN
            valueFrom:
              secretKeyRef:
                name: hf-secret
                key: HF_TOKEN
          - name: VLLM_ADDITIONAL_ARGS
            value: "--disable-uvicorn-access-log --max-model-len=4096 --gpu-memory-utilization=0.9"
        resources:
          limits:
            cpu: '1'
            memory: 8Gi
            nvidia.com/gpu: "1"
          requests:
            cpu: '1'
            memory: 8Gi
            nvidia.com/gpu: "1"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
            scheme: HTTPS
          initialDelaySeconds: 120
          periodSeconds: 30
          timeoutSeconds: 30
          failureThreshold: 5
EOF
----

**Key configuration elements**:

* **`replicas: 4`**: Deploys 4 vLLM inference pool instances
* **`serving.kserve.io/storageSecretName`**: Annotation that tells KServe to mount `hf-secret` into storage-initializer
* **`containers[main].env.HF_TOKEN`**: Configures HuggingFace token for vLLM runtime and tokenizer access
* **`precise-prefix-cache-scorer`**: The intelligent routing plugin (weight: 3)
* **`queue-scorer`**: Balances queue depth (weight: 2)
* **`kv-cache-utilization-scorer`**: Optimizes cache memory usage (weight: 2)
* **Plugin weights**: Higher weight = more influence on routing decisions

=== Apply the LLMInferenceService

[source,bash]
----
oc apply -f llm-d-inference.yaml
----

=== Verify HuggingFace token configuration

Before the deployment completes, verify the HF_TOKEN is properly configured:

[source,bash]
----
# Wait for the deployment to be created
sleep 10

# Check that storage-initializer has HF_TOKEN environment variable
oc get deployment llama-llm-d-kserve -o jsonpath='{.spec.template.spec.initContainers[0].env[*].name}' | grep -q HF_TOKEN && echo "✓ HF_TOKEN configured in storage-initializer" || echo "✗ HF_TOKEN missing in storage-initializer"

# Check that main container has HF_TOKEN environment variable
oc get deployment llama-llm-d-kserve -o jsonpath='{.spec.template.spec.containers[0].env[*].name}' | grep -q HF_TOKEN && echo "✓ HF_TOKEN configured in main container" || echo "✗ HF_TOKEN missing in main container"
----

You should see both checks pass with ✓ marks.

=== Monitor deployment progress

Watch the LLMInferenceService deployment:

[source,bash]
----
# Watch LLMInferenceService status
oc get llminferenceservice llama-llm-d -w

# Watch inference pool pods being created (4 replicas)
oc get pods -l serving.kserve.io/inferenceservice=llama-llm-d -w
----

The deployment will go through these states:
1. **Creating**: LLMInferenceService resource accepted
2. **InProgress**: Router and inference pool pods deploying, models downloading
3. **Ready**: All 4 inference pool instances ready, router active

This can take 10-15 minutes as each instance downloads the model.

**What gets deployed**:
* **1 Router pod**: The llm-d scheduler with precise-prefix-cache-aware routing
* **4 Inference pool pods**: vLLM instances (1 per GPU) managed by llm-d
* **Services and Routes**: Automatically created for external access

**Check storage-initializer logs** (optional):

[source,bash]
----
# Get a pod name
POD=$(oc get pods -l serving.kserve.io/inferenceservice=llama-llm-d -o jsonpath='{.items[0].metadata.name}')

# View storage-initializer logs
oc logs $POD -c storage-initializer
----

You may see permission warnings like "Could not set the permissions on the file" - these are harmless and the download will continue successfully.

=== Verify llm-d is running

Once the LLMInferenceService shows `Ready`, get the inference URL:

[source,bash]
----
# Get the llm-d inference URL
LLMD_URL=$(oc get llminferenceservice llama-llm-d -o jsonpath='{.status.url}')
echo "llm-d URL: ${LLMD_URL}"

# Test with a simple request
curl -k ${LLMD_URL}/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Meta-Llama-3.1-8B-Instruct",
    "prompt": "San Francisco is a",
    "max_tokens": 20
  }'
----

Expected output: llm-d's router analyzes the prompt, routes to an inference pool instance based on cache state, and returns a completion.

=== Verify intelligent routing is active

Check the router logs to see precise-prefix-cache-scorer in action:

[source,bash]
----
# Get router pod name
ROUTER_POD=$(oc get pods -l app.kubernetes.io/component=router -o jsonpath='{.items[0].metadata.name}')

# View router logs showing routing decisions
oc logs $ROUTER_POD --tail=50

# You should see log lines indicating:
# - Prefix similarity analysis
# - Scoring plugin results (queue, kv-cache-utilization, precise-prefix-cache)
# - Backend selection based on cache awareness
----

=== Troubleshooting

**Issue**: LLMInferenceService stuck in "InProgress"
**Solution**:
[source,bash]
----
# Check LLMInferenceService status
oc describe llminferenceservice llama-llm-d

# Check router pod status
oc get pods -l app.kubernetes.io/component=router
oc describe pod -l app.kubernetes.io/component=router

# Check inference pool pods
oc get pods -l serving.kserve.io/inferenceservice=llama-llm-d
oc logs -l serving.kserve.io/inferenceservice=llama-llm-d --tail=100

# Common issues:
# - GPU quota: Verify 4 GPUs available
# - Image pull: Check image access and pull secrets
# - Model download: Check HuggingFace access (may need HF_TOKEN)
----

**Issue**: Model download timeout
**Solution**: If models are pre-cached on PVC, update the `uri`:
[source,yaml]
----
spec:
  model:
    uri: pvc://model-cache/llama-3.1-8b
----

== Exercise 2: Configure llm-d monitoring

Set up comprehensive monitoring for llm-d router and inference pool metrics.

=== Create ServiceMonitor for llm-d router

Monitor the router's scheduling decisions and cache hit rates:

[source,bash]
----
cat > llm-d-router-servicemonitor.yaml << 'EOF'
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: llm-d-router-metrics
spec:
  selector:
    matchLabels:
      app.kubernetes.io/component: router
  endpoints:
    - port: metrics
      path: /metrics
      interval: 30s
      scheme: https
      tlsConfig:
        insecureSkipVerify: true
EOF
----

Apply the ServiceMonitor:

[source,bash]
----
oc apply -f llm-d-router-servicemonitor.yaml
----

=== Create ServiceMonitor for inference pools

Monitor the vLLM inference pool instances:

[source,bash]
----
cat > llm-d-pool-servicemonitor.yaml << 'EOF'
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: llm-d-pool-metrics
spec:
  selector:
    matchLabels:
      serving.kserve.io/inferenceservice: llama-llm-d
  endpoints:
    - port: metrics
      path: /metrics
      interval: 30s
      scheme: https
      tlsConfig:
        insecureSkipVerify: true
EOF
----

Apply the ServiceMonitor:

[source,bash]
----
oc apply -f llm-d-pool-servicemonitor.yaml
----

=== Import llm-d Grafana dashboard

Import the llm-d dashboard to visualize:
* Request routing decisions
* Cache hit rates per backend
* Latency per backend
* Throughput and load distribution

1. Open Grafana
2. Navigate to **Dashboards → Import**
3. Upload `llm-d-dashboard.json` (provided in workshop materials)
4. Select Prometheus data source
5. Click **Import**

Key panels in llm-d dashboard:
* **Cache hit rate**: Percentage of requests benefiting from KV cache
* **Routing distribution**: Which backend received each request
* **Backend utilization**: Load per vLLM instance
* **Prefix similarity**: How similar incoming requests are
* **Throughput comparison**: Aggregate throughput across all backends

=== Verify metrics are flowing

[source,bash]
----
# Get router pod name
ROUTER_POD=$(oc get pods -l app.kubernetes.io/component=router -o jsonpath='{.items[0].metadata.name}')

# Port-forward to router metrics endpoint
oc port-forward $ROUTER_POD 9090:9090 &

# Fetch router metrics
curl -k https://localhost:9090/metrics | grep -E '(scheduler|cache|queue)'

# Stop port-forward
pkill -f "port-forward"
----

Look for metrics like:
* `scheduler_endpoint_picker_scoring_duration_seconds`: Routing decision time
* `scheduler_queue_depth`: Queue length per inference pool
* Cache-related metrics from precise-prefix-cache-scorer
* `vllm:num_requests_running` and `vllm:gpu_cache_usage_perc` from inference pools

== Exercise 3: Run llm-d benchmarks

Now run the same two scenarios against llm-d to measure P95/P99 tail latency improvements from cache-aware routing.

**Goal**: Demonstrate that intelligent routing dramatically reduces tail latency for your most frustrated users.

=== Warm up the system

Before benchmarking, send some requests to prime the caches:

[source,bash]
----
# Send warmup requests with shared prefixes
for i in {1..50}; do
  curl -k https://${LLMD_URL}/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
      "model": "meta-llama/Meta-Llama-3.1-8B-Instruct",
      "prompt": "You are a helpful customer service agent. User asks: How can I help you?",
      "max_tokens": 30
    }' &
done
wait
----

This primes the KV caches with the system prompt across llm-d's routing decisions.

=== Run llm-d benchmark

Run the same benchmark you performed in Modules 2 and 3, now against llm-d:

[source,bash]
----
# Get llm-d URL
LLMD_URL=$(oc get llminferenceservice llama-llm-d -o jsonpath='{.status.url}')
echo "LLMD_URL: ${LLMD_URL}"

# Run benchmark with sweep profile
guidellm benchmark \
  --target "${LLMD_URL}" \
  --profile sweep \
  --max-seconds 60 \
  --processor meta-llama/Meta-Llama-3.1-8B-Instruct \
  --data "prompt_tokens=256,output_tokens=128"
----

=== Compare results

Compare your P95 TTFT results across all three configurations:

* **Single GPU (Module 2)**: Your baseline P95 TTFT
* **4 GPUs naive (Module 3)**: Your scaled P95 TTFT
* **4 GPUs llm-d (this module)**: Your llm-d P95 TTFT

**Why llm-d works**:
* llm-d routes requests with similar prefixes to the same backend
* Cache hits eliminate redundant prefill computation
* P95/P99 users benefit from cached prefill
* **Your most frustrated users now get much faster responses**

**The key difference**: llm-d routes similar requests to the same backend, ensuring cache hits. Naive scaling routes randomly, causing cache misses and forcing recomputation.

== Exercise 4: Analyze llm-d performance gains

Compare all three configurations: single GPU, naive 4 GPU, and llm-d 4 GPU by analyzing your benchmark results.

== Exercise 5: Monitor cache effectiveness in Grafana

Watch llm-d's cache-aware routing directly improve tail latency in real-time:

1. Open Grafana llm-d dashboard
2. Run the benchmark
3. Observe in real-time:
   * **Cache hit rate**: Should increase for workloads with shared prefixes
   * **Routing decisions**: Requests with similar prefixes consistently route to same backends
   * **Backend utilization**: Uneven but intentional (hot caches get more traffic)
   * **P95/P99 latency**: Compare to your Module 3 results

**Key metrics to watch**:
* **Cache hit rate climbing**: Starts low, climbs as system warms up
* **Routing affinity**: Similar prompts cluster on same backend

Compare this to the naive scaling dashboard from Module 3:
* **Naive**: All backends equally loaded, low cache hit rate
* **llm-d**: Uneven but intentional distribution, higher cache hit rate

Take screenshots showing:
* Cache hit rate over time
* P95/P99 latency comparison
* Routing distribution showing cache affinity

== Exercise 6: Understanding when llm-d provides value

llm-d's benefit depends on workload characteristics:

=== High prefix similarity (best case)

Workloads with shared system prompts or common prefixes benefit most from llm-d. This includes:
* Customer service chatbots with shared system prompts
* Multi-turn conversations
* Applications with consistent instruction prefixes

=== Low prefix similarity

For completely unique prompts with no shared prefixes:
* Cache hit rate will be low
* Performance similar to naive scaling
* llm-d doesn't hurt, but doesn't help much

=== Partial prefix sharing

Mixed workloads with some shared prefixes:
* Moderate cache hit rates
* Proportional P95/P99 improvements
* Even partial prefix sharing provides measurable benefits

**Key insight**: The more prefix similarity in your workload, the more llm-d helps reduce tail latency.

== Module 4 summary

**What you accomplished:**
* Deployed llm-d orchestrator with Precise Prefix Cache Aware Routing
* Configured llm-d to intelligently manage 4 vLLM backend instances
* Set up comprehensive monitoring with Grafana to track cache hits and P95/P99 latency
* Ran benchmarks demonstrating tail latency improvements with cache-aware routing
* Proved that intelligent routing solves the problem naive scaling couldn't

**Key takeaways - The Tail Latency Breakthrough:**

Compare your results across all three configurations:

[source,text]
----
=== Your Results Summary ===

Single GPU (Module 2):
- P95 TTFT: _______ (your recorded value)

4 GPUs Naive (Module 3):
- P95 TTFT: _______ (your recorded value)
- Improvement over single GPU: _______ %

4 GPUs llm-d (Module 4):
- P95 TTFT: _______ (your recorded value)
- Improvement over naive: _______ %
----

**The critical insight:**
* Same hardware (4 GPUs)
* Same capacity (requests per second)
* But **dramatically different customer experience** for P95/P99 users
* **Cache-aware routing reduces the tail latency problem**

**Why llm-d works for tail latency:**
* **Analyzes prompt prefixes** and routes similar requests to same backend
* **Maximizes KV cache hits**: Requests benefit from cached prefill
* **Eliminates redundant computation**: Shared prefixes cached, not recomputed
* **Routes multi-turn to same backend**: Conversation history stays cached
* **Directly attacks the P95/P99 problem**: Cache hits mean your worst-case users avoid queuing

**When llm-d provides value:**
* Workloads with shared prefixes (system prompts, common contexts)
* Multi-turn conversations requiring context reuse
* Any scenario where cache misses cause high P95/P99 latency
* When customer experience (not just capacity) matters

**Workshop complete!**

You've successfully demonstrated that intelligent cache-aware routing reduces tail latency for your most frustrated users. You now have the data and experience to advocate for llm-d deployment in production environments where P95/P99 latency matters.
