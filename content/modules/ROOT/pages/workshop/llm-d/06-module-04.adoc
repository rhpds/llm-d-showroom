= Module 4: Deploy llm-d & Benchmark
:source-highlighter: rouge
:toc: macro
:toclevels: 2

You've proven that 4 GPUs with naive scaling don't significantly reduce P95/P99 tail latency—your most frustrated users still experience slow responses. Now it's time to deploy llm-d with Prefix Cache Aware Routing to dramatically reduce tail latency through intelligent cache reuse.

In this module, you'll deploy llm-d to orchestrate your 4 vLLM instances with cache-aware routing, run the same benchmarks, and measure the P95/P99 improvements.

== Learning objectives
By the end of this module, you'll be able to:

* Deploy llm-d with Prefix Cache Aware Routing configuration
* Configure llm-d to orchestrate multiple vLLM backend instances
* Use the Grafana dashboard to monitor cache hit rates
* Execute benchmarks across both scenarios and measure P95/P99 latency reductions
* Quantify customer experience improvements with real tail latency data

== Understanding llm-d deployment architecture

Before deploying, understand how llm-d integrates with your existing vLLM instances:

[source,text]
----
                    Client Requests
                           |
                           v
                   ┌───────────────┐
                   │  llm-d Pod    │
                   │ (Orchestrator)│
                   └───────┬───────┘
                           |
        ┌──────────────────┼──────────────────┐
        |                  |                  |
        v                  v                  v
  ┌──────────┐       ┌──────────┐       ┌──────────┐
  │ vLLM     │       │ vLLM     │       │ vLLM     │
  │ Instance │       │ Instance │       │ Instance │
  │ 1 (GPU1) │       │ 2 (GPU2) │  ...  │ 4 (GPU4) │
  └──────────┘       └──────────┘       └──────────┘

llm-d routing logic:
1. Receives request from client
2. Analyzes prompt prefix
3. Computes similarity to cached prefixes per backend
4. Routes to backend with highest cache hit probability
5. Returns response to client
----

**Key insight**: Your 4 vLLM instances keep running. llm-d sits in front as an intelligent reverse proxy.

== Exercise 1: Deploy llm-d with LLMInferenceService

=== Clean up scaled-vLLM deployment

First, remove your inference service deployment, because llm-d will manage the vLLM deployment now.

[.console-input]
[source,bash]
----
# Delete previous InferenceService
oc delete inferenceservice llama-vllm-scaled

# Verify deletion
oc get inferenceservice
----

=== Understanding LLMInferenceService

llm-d uses the **LLMInferenceService** custom resource to deploy an intelligent orchestration layer with precise-prefix-cache-aware routing. This is a Kubernetes-native approach that:

* Deploys multiple vLLM inference pool replicas
* Configures the llm-d router with intelligent scheduling plugins
* Enables **precise-prefix-cache-scorer** for cache-aware routing

=== Create Gateway Object

A GatewayClass and a Gateway for the inference service should be created.

[.console-input]
[source,bash]
----
cat << 'EOF' | oc apply -f-
---
apiVersion: gateway.networking.k8s.io/v1
kind: GatewayClass
metadata:
  name: openshift-ai-inference
spec:
  controllerName: openshift.io/gateway-controller/v1
---
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: openshift-ai-inference
  namespace: openshift-ingress
spec:
  gatewayClassName: openshift-default
  listeners:
    - name: http
      port: 80
      protocol: HTTP
      allowedRoutes:
        namespaces:
          from: All
EOF
----

=== Create the LLMInferenceService

Create the llm-d configuration with 4 replicas and precise-prefix-cache-aware routing:

[.console-input]
[source,bash]
----
cat << 'EOF' | oc apply -f-
apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceService
metadata:
  name: llama-llm-d
  annotations:
    opendatahub.io/model-type: generative
    openshift.io/display-name: llama-llm-d
    security.opendatahub.io/enable-auth: 'false'
    prometheus.io/path: /metrics
    prometheus.io/port: "8000"
spec:
  replicas: 4
  model:
    uri: oci://registry.redhat.io/rhelai1/modelcar-llama-3-1-8b-instruct-fp8-dynamic:1.5
    name: llama-3-1-8b-instruct-fp8
  router:
    scheduler:
      template:
        containers:
          - name: main
            env:
              - name: TOKENIZER_CACHE_DIR
                value: /tmp/tokenizer-cache
              - name: HF_HOME
                value: /tmp/tokenizer-cache
              - name: TRANSFORMERS_CACHE
                value: /tmp/tokenizer-cache
              - name: XDG_CACHE_HOME
                value: /tmp
            args:
              - --pool-group
              - inference.networking.x-k8s.io
              - '--pool-name'
              - '{{ ChildName .ObjectMeta.Name `-inference-pool` }}'
              - '--pool-namespace'
              - '{{ .ObjectMeta.Namespace }}'
              - '--zap-encoder'
              - json
              - '--grpc-port'
              - '9002'
              - '--grpc-health-port'
              - '9003'
              - '--secure-serving'
              - '--model-server-metrics-scheme'
              - https
              - '--config-text'
              - |
                apiVersion: inference.networking.x-k8s.io/v1alpha1
                kind: EndpointPickerConfig
                plugins:
                - type: single-profile-handler
                - type: queue-scorer
                - type: kv-cache-utilization-scorer
                - type: prefix-cache-scorer
                schedulingProfiles:
                - name: default
                  plugins:
                  - pluginRef: queue-scorer
                    weight: 2
                  - pluginRef: kv-cache-utilization-scorer
                    weight: 2
                  - pluginRef: prefix-cache-scorer
                    weight: 3
            volumeMounts:
              - name: tokenizer-cache
                mountPath: /tmp/tokenizer-cache
              - name: cachi2-cache
                mountPath: /cachi2
        volumes:
          - name: tokenizer-cache
            emptyDir: {}
          - name: cachi2-cache
            emptyDir: {} 
    route: {}
    gateway: {}
  template:
    tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
    containers:
      - name: main
        env:
          - name: VLLM_ADDITIONAL_ARGS
            value: "--disable-uvicorn-access-log --max-model-len=16000 --gpu-memory-utilization=0.9"
        resources:
          limits:
            cpu: '1'
            memory: 8Gi
            nvidia.com/gpu: "1"
          requests:
            cpu: '1'
            memory: 8Gi
            nvidia.com/gpu: "1"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
            scheme: HTTPS
          initialDelaySeconds: 120
          periodSeconds: 30
          timeoutSeconds: 30
          failureThreshold: 5
EOF
----

**Key configuration elements**:

* **`replicas: 4`**: Deploys 4 vLLM inference pool instances
* **`model.uri`**: Uses OCI registry to pull the pre-packaged FP8 quantized model
* **`precise-prefix-cache-scorer`**: The intelligent routing plugin (weight: 3)
* **`queue-scorer`**: Balances queue depth (weight: 2)
* **`kv-cache-utilization-scorer`**: Optimizes cache memory usage (weight: 2)
* **Plugin weights**: Higher weight = more influence on routing decisions

=== Monitor deployment progress

Watch the LLMInferenceService deployment:

[.console-input]
[source,bash]
----
# Watch LLMInferenceService status
oc get llminferenceservice llama-llm-d -w

# Watch inference pool pods being created (4 replicas)
oc get pods -l kserve.io/component=workload -w
----

The deployment will go through these states:

1. **Creating**: LLMInferenceService resource accepted
2. **InProgress**: Router and inference pool pods deploying, models downloading
3. **Ready**: All 4 inference pool instances ready, router active

This can take a few minutes as each instance pulls the model from the OCI registry.

**What gets deployed**:

* **1 Router pod**: The llm-d scheduler with precise-prefix-cache-aware routing
* **4 Inference pool pods**: vLLM instances (1 per GPU) managed by llm-d
* **Services **: Automatically created for access

=== Verify llm-d is running

Once the LLMInferenceService shows `Ready`, get the inference URL:

[.console-input]
[source,bash]
----
# Get the llm-d inference URL
LLMD_URL=$(oc get llminferenceservice llama-llm-d -o jsonpath='{.status.url}')
echo "llm-d URL: ${LLMD_URL}"

# Test with a simple request
curl ${LLMD_URL}/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama-3-1-8b-instruct-fp8",
    "prompt": "San Francisco is a",
    "max_tokens": 50,
    "temperature": 0.7
  }' | jq .
----

Expected output: llm-d's router analyzes the prompt, routes to an inference pool instance based on cache state, and returns a completion.

[.console-output]
[source,bash]
----
{
  "choices": [
    {
      "finish_reason": "length",
      "index": 0,
      "logprobs": null,
      "prompt_logprobs": null,
      "prompt_token_ids": null,
      "stop_reason": null,
      "text": " top tourist destination in the United States, attracting millions of visitors each year. From its iconic Golden Gate Bridge to its vibrant neighborhoods like Haight-Ashbury and Fisherman's Wharf, there's no shortage of things to see and do in this",
      "token_ids": null
    }
  ],
  "created": 1765225596,
  "id": "cmpl-53eaaee5-9499-4343-a20e-d42776e3a49e",
  "kv_transfer_params": null,
  "model": "llama-3-1-8b-instruct-fp8",
  "object": "text_completion",
  "service_tier": null,
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 50,
    "prompt_tokens": 5,
    "prompt_tokens_details": null,
    "total_tokens": 55
  }
}
----

== Exercise 2: Configure llm-d monitoring

Set up comprehensive monitoring for llm-d router and inference pool metrics.

=== Create ServiceMonitor for llm-d router

Monitor the router's scheduling decisions and cache hit rates:

[.console-input]
[source,bash]
----
cat << 'EOF' | oc apply -f-
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: llm-d-router-metrics
  labels:
    monitoring.opendatahub.io/scrape: "true"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/component: llminferenceservice-router-scheduler
  endpoints:
    - port: metrics
      path: /metrics
      interval: 30s
      scheme: https
      tlsConfig:
        insecureSkipVerify: true
EOF
----

=== Create ServiceMonitor for inference pools

Monitor the vLLM inference pool instances:

[.console-input]
[source,bash]
----
cat << 'EOF' | oc apply -f-
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: llm-d-pool-metrics
  labels:
    monitoring.opendatahub.io/scrape: "true"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/component: llminferenceservice-workload
  endpoints:
    - port: https
      path: /metrics
      interval: 30s
      scheme: https
      tlsConfig:
        insecureSkipVerify: true
EOF
----

== Exercise 3: Run llm-d benchmarks

We're run the same scenario against llm-d to measure P95/P99 tail latency improvements from cache-aware routing.

**Goal**: Demonstrate that intelligent routing dramatically reduces tail latency for your most frustrated users.


=== Run llm-d benchmark

Run the same benchmark you performed in Modules 2 and 3, now against llm-d:

[.console-input]
[source,bash]
----
# Create the benchmark job
cat << EOF | oc apply -f-
apiVersion: batch/v1
kind: Job
metadata:
  name: custom-benchmark-vllm-scaled
spec:
  backoffLimit: 4
  template:
    spec:
      serviceAccountName: default
      restartPolicy: Never
      containers:
        - name: benchmark
          image: quay.io/hayesphilip/multi-turn-benchmark:0.0.1
          args:
            - http://openshift-ai-inference-openshift-default.openshift-ingress.svc.cluster.local/llm-d-project/llama-llm-d/v1
            - --parallel=9
EOF

=== Compare results

When your benchmark finishes, you should again see a results summary similar to the following:

[.console-output]
[source,bash]
----


----

Compare your P95 TTFT results across all three configurations:

* **Single GPU (Module 2)**: Your baseline P95 TTFT
* **4 GPUs naive (Module 3)**: Your scaled P95 TTFT
* **4 GPUs llm-d (this module)**: Your llm-d P95 TTFT

**Why llm-d works**:
* llm-d routes requests with similar prefixes to the same backend
* Cache hits eliminate redundant prefill computation
* P95/P99 users benefit from cached prefill
* **Your most frustrated users now get much faster responses**

**The key difference**: llm-d routes similar requests to the same backend, ensuring cache hits. Naive scaling routes randomly, causing cache misses and forcing recomputation.



== Module 4 summary

**What you accomplished:**
* Deployed llm-d orchestrator with Precise Prefix Cache Aware Routing
* Configured llm-d to intelligently manage 4 vLLM backend instances
* Set up comprehensive monitoring with Grafana to track cache hits and P95/P99 latency
* Ran benchmarks demonstrating tail latency improvements with cache-aware routing
* Proved that intelligent routing solves the problem naive scaling couldn't

**Key takeaways - The Tail Latency Breakthrough:**

Compare your results across all three configurations:

[.console-input]
[source,text]
----
=== Your Results Summary ===

Single GPU (Module 2):
- P95 TTFT: _______ (your recorded value)

4 GPUs Naive (Module 3):
- P95 TTFT: _______ (your recorded value)
- Improvement over single GPU: _______ %

4 GPUs llm-d (Module 4):
- P95 TTFT: _______ (your recorded value)
- Improvement over naive: _______ %
----

**The critical insight:**
* Same hardware (4 GPUs)
* Same capacity (requests per second)
* But **dramatically different customer experience** for P95/P99 users
* **Cache-aware routing reduces the tail latency problem**

**Why llm-d works for tail latency:**
* **Analyzes prompt prefixes** and routes similar requests to same backend
* **Maximizes KV cache hits**: Requests benefit from cached prefill
* **Eliminates redundant computation**: Shared prefixes cached, not recomputed
* **Routes multi-turn to same backend**: Conversation history stays cached
* **Directly attacks the P95/P99 problem**: Cache hits mean your worst-case users avoid queuing

**When llm-d provides value:**
* Workloads with shared prefixes (system prompts, common contexts)
* Multi-turn conversations requiring context reuse
* Any scenario where cache misses cause high P95/P99 latency
* When customer experience (not just capacity) matters

**Workshop complete!**

You've successfully demonstrated that intelligent cache-aware routing reduces tail latency for your most frustrated users. You now have the data and experience to advocate for llm-d deployment in production environments where P95/P99 latency matters.
