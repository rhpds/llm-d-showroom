= Workshop details
:toc:
:toc-placement: preamble
:icons: font

== Timing and schedule

=== Full workshop (120 minutes / 2 hours)
* **Module 1**: Introduction & Core Concepts (20 minutes)
* **Module 2**: Deploy Model with vLLM & Benchmark (25 minutes)
* **Module 3**: Scale vLLM & Compare Performance (25 minutes)
* **Module 4**: Deploy llm-d & Benchmark (35 minutes)
* **Module 5**: Recap & Customer Value (15 minutes)

=== Abbreviated workshop (90 minutes)
If time is constrained, consider these adjustments:
* **Module 1**: Introduction & Core Concepts (15 minutes) - Focus on key concepts only
* **Module 2**: Deploy Model with vLLM & Benchmark (20 minutes) - Use pre-deployed baseline
* **Module 3**: Scale vLLM & Compare Performance (20 minutes) - Show results vs. hands-on
* **Module 4**: Deploy llm-d & Benchmark (25 minutes) - Focus on llm-d deployment
* **Module 5**: Recap & Customer Value (10 minutes) - Quick summary

== Technical requirements

=== Software versions
* **OpenShift AI** 3.0 or later (includes KServe)
* **vLLM** TBD
* **llm-d** TBD
* **GuideLLM** TBD
* **Grafana** TBD
* **Python** TBD


=== Model requirements
* **Primary model**: Meta-Llama-3.1-8B-Instruct
* **Model storage**: HuggingFace Hub access or pre-cached in cluster PVC

=== Infrastructure provided
* **GPU nodes**: 4 NVIDIA L4 GPUs
* **GPU memory**: 24GB+ per GPU for 8B models
* **CPU/Memory**: 8 CPU cores, 32GB RAM 
* **Storage**: 100GB+ for model weights and cache
* **Network**: Low-latency interconnect for distributed inference (InfiniBand or RoCE preferred)

=== Environment 

TBD

== Environment architecture

=== OpenShift AI components
* **KServe**: Model serving platform managing InferenceServices
* **ServingRuntime**: vLLM runtime configuration for LLM inference
* **GPU Operator**: NVIDIA GPU resource management
* **Prometheus**: Metrics collection from vLLM and llm-d
* **Grafana**: Visualization dashboards for inference metrics
