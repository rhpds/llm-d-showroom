= Workshop details
:toc:
:toc-placement: preamble
:icons: font

== Timing and schedule

=== Full workshop (120 minutes / 2 hours)
* **Module 1**: Introduction & Core Concepts (20 minutes)
* **Module 2**: Deploy Model with vLLM & Benchmark (25 minutes)
* **Module 3**: Scale vLLM & Compare Performance (25 minutes)
* **Module 4**: Deploy llm-d & Benchmark (35 minutes)
* **Module 5**: Recap & Customer Value (15 minutes)

=== Abbreviated workshop (90 minutes)
If time is constrained, consider these adjustments:
* **Module 1**: Introduction & Core Concepts (15 minutes) - Focus on key concepts only
* **Module 2**: Deploy Model with vLLM & Benchmark (20 minutes) - Use pre-deployed baseline
* **Module 3**: Scale vLLM & Compare Performance (20 minutes) - Show results vs. hands-on
* **Module 4**: Deploy llm-d & Benchmark (25 minutes) - Focus on llm-d deployment
* **Module 5**: Recap & Customer Value (10 minutes) - Quick summary

== Technical requirements

=== Software versions
* **OpenShift AI** 3.0 or later (includes KServe)
* **vLLM** TBD
* **llm-d** TBD
* **GuideLLM** TBD
* **Grafana** TBD
* **Python** TBD


=== Model requirements
* **Primary model**: Meta-Llama-3.1-8B-Instruct, quantized to FP8
* **Model storage**: Model Car images provided by Red Hat AI

=== Infrastructure provided
* **GPU node**: 4 NVIDIA L4 GPUs
* **GPU memory**: 24GB per GPU
* **CPU/Memory**: 48 vCPU and 192 GB of RAM
* **Storage**: 1.5TB for model weights and image cache
* **Network**: 40Gbps networking

=== Environment

TBD

== Environment architecture

=== OpenShift AI components
* **KServe**: Model serving platform managing ServingRuntimes and InferenceServices
* **GPU Operator**: NVIDIA GPU resource management
* **Prometheus (User Workload Monitoring)**: Metrics collection from vLLM and llm-d
* **Grafana**: Visualization dashboards for inference metrics
