= Module 2: Deploy Model with vLLM & Benchmark
:source-highlighter: rouge
:toc: macro
:toclevels: 2

Now it's time to establish your performance baseline. Before evaluating llm-d's improvements, you need quantifiable metrics from a single-GPU vLLM deployment.

In this module, you'll deploy Llama 3.1 8B with vLLM on a single GPU using KServe in OpenShift AI, configure monitoring, and run comprehensive benchmarks with GuideLLM.

== Learning objectives
By the end of this module, you'll be able to:

* Deploy a generative AI model using KServe and vLLM ServingRuntime in OpenShift AI
* Configure Grafana dashboard to collect vLLM performance metrics
* Execute GuideLLM benchmarks to measure TTFT, ITL, and throughput
* Analyze results to identify single-instance performance limits

== Exercise 1: Deploy vLLM InferenceService

ParasolCloud needs a baseline measurement of single-GPU inference performance. You'll deploy Llama 3.1 8B Instruct on one GPU to establish this baseline.

=== Verify your environment

First, confirm your OpenShift AI environment is ready:

[source,bash]
----
# Verify you're logged into OpenShift
oc whoami

# Check your assigned namespace
oc project

# Verify GPU nodes are available
oc get nodes -l nvidia.com/gpu.present=true

# Check GPU quota in your namespace
oc get resourcequota
----

Expected output: Your username, namespace, GPU nodes listed, and GPU quota showing available resources.

=== Create the vLLM ServingRuntime

KServe uses ServingRuntimes to define how models are served. You'll create a vLLM ServingRuntime configuration.

Create a file named `vllm-runtime.yaml`:

[source,bash]
----
cat > vllm-runtime.yaml << 'EOF'
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: vllm-runtime
spec:
  annotations:
    opendatahub.io/kserve-runtime: vllm
    prometheus.io/path: /metrics
    prometheus.io/port: '8080'
  containers:
    - args:
        - '--port=8080'
        - '--model=/mnt/models'
        - '--served-model-name={{.Name}}'
        - '--max-model-len=8000'
      command:
        - python
        - '-m'
        - vllm.entrypoints.openai.api_server
      env:
        - name: HF_HOME
          value: /tmp/hf_home
      image: 'registry.redhat.io/rhaiis/vllm-cuda-rhel9@sha256:ad756c01ec99a99cc7d93401c41b8d92ca96fb1ab7c5262919d818f2be4f3768'
      name: kserve-container
      ports:
        - containerPort: 8080
          protocol: TCP
  multiModel: false
  supportedModelFormats:
    - autoSelect: true
      name: vLLM
EOF
----

Apply the ServingRuntime:

[source,bash]
----
oc apply -f vllm-runtime.yaml
----

Verify the ServingRuntime was created:

[source,bash]
----
oc get servingruntimes
----

=== Create the InferenceService

Now deploy the Llama 3.1 8B model using the vLLM runtime.

[source,bash]
----
# Create secret with your HuggingFace token
oc create secret generic hf-token \
  --from-literal=HF_TOKEN=your_token_here
----

Create the InferenceService configuration:

[source,bash]
----
cat > llama-vllm-single.yaml << 'EOF'
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llama-vllm-single
  annotations:
    serving.kserve.io/deploymentMode: RawDeployment
spec:
  predictor:
    automountServiceAccountToken: false
    imagePullSecrets:
      - name: secret-pafskz
    maxReplicas: 1
    minReplicas: 1
    model:
      modelFormat:
        name: vLLM
      name: ''
      resources:
        limits:
          cpu: '2'
          memory: 4Gi
          nvidia.com/gpu: '1'
        requests:
          cpu: '2'
          memory: 4Gi
          nvidia.com/gpu: '1'
      runtime: llama
      storageUri: 'oci://registry.redhat.io/rhelai1/modelcar-llama-3-1-8b-instruct-fp8-dynamic:1.5'
EOF
----

Apply the InferenceService:

[source,bash]
----
oc apply -f llama-vllm-single.yaml
----

=== Monitor deployment progress

Watch the InferenceService status:

[source,bash]
----
# Watch InferenceService status
oc get inferenceservice llama-vllm-single -w

# In another terminal, watch pod creation
oc get pods -l serving.kserve.io/inferenceservice=llama-vllm-single -w
----

The deployment will go through these states:
1. **Unknown**: Initial state
2. **InProgress**: Pod creating, model downloading
3. **Ready**: Model loaded and serving

This can take 5-10 minutes depending on model download speed.

Check logs if there are issues:

[source,bash]
----
# View container logs
oc logs -l serving.kserve.io/inferenceservice=llama-vllm-single -f
----

=== Test the deployment

Once the InferenceService shows `Ready`, test it:

[source,bash]
----
# Get the inference URL
INFERENCE_URL=$(oc get inferenceservice llama-vllm-single -o jsonpath='{.status.url}'):8080
echo $INFERENCE_URL

# Test with curl
curl -k ${INFERENCE_URL}/v1/models

# Send a test request
curl -k ${INFERENCE_URL}/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama-vllm-single",
    "prompt": "San Francisco is a",
    "max_tokens": 50,
    "temperature": 0.7
  }'
----

Expected output: JSON response with generated text completion.

=== Troubleshooting

**Issue**: InferenceService stuck in "Unknown" or "InProgress"
**Solution**:
[source,bash]
----
# Check pod status
oc get pods -l serving.kserve.io/inferenceservice=llama-vllm-single

# Check events
oc get events --sort-by='.lastTimestamp' | grep llama-vllm-single

# Common issues:
# - Image pull: Check image name and pull secrets
# - GPU quota: Verify GPU resources available
----

== Exercise 2: Configure Grafana monitoring

To understand vLLM performance, you need metrics visualization. You'll configure a Grafana dashboard to track vLLM inference metrics.

=== Expose vLLM metrics endpoint

vLLM exposes Prometheus metrics on its metrics port. Verify the service is exposing metrics:

[source,bash]
----
# Find the vLLM service
oc get svc -l serving.kserve.io/inferenceservice=llama-vllm-single

# Port-forward to test metrics endpoint
oc port-forward svc/llama-vllm-single-predictor 8080:8080 &

# Fetch metrics (in another terminal or after port-forward runs)
curl http://localhost:8080/metrics | grep vllm

# Stop port-forward
pkill -f "port-forward"
----

You should see vLLM-specific metrics like:
* `vllm:num_requests_running`
* `vllm:num_requests_waiting`
* `vllm:gpu_cache_usage_perc`
* `vllm:avg_generation_throughput_toks_per_s`

=== Create ServiceMonitor for Prometheus

To collect vLLM metrics in Prometheus, create a ServiceMonitor:

[source,bash]
----
cat > vllm-servicemonitor.yaml << 'EOF'
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: vllm-metrics
spec:
  selector:
    matchLabels:
      serving.kserve.io/inferenceservice: llama-vllm-single
  endpoints:
    - port: http
      path: /metrics
      interval: 30s
EOF
----

Apply the ServiceMonitor:

[source,bash]
----
oc apply -f vllm-servicemonitor.yaml
----

=== Access Grafana dashboard

Your workshop environment should have a pre-configured Grafana instance. Access it:

[source,bash]
----
# Get Grafana route
oc get route grafana -n openshift-monitoring

# Open in browser (URL from command above)
----

Login with your OpenShift credentials.

=== Import vLLM dashboard

Import the vLLM monitoring dashboard:

1. Navigate to **Dashboards â†’ Import**
2. Upload the provided `vllm-dashboard.json` or paste the JSON content
3. Select your Prometheus data source
4. Click **Import**

Key panels to monitor:
* **Requests per second**: Current throughput
* **Active requests**: Concurrent requests being processed
* **GPU cache usage**: Memory utilization
* **Generation throughput**: Tokens generated per second
* **Request latency**: P50, P95, P99 latencies

NOTE: Dashboard files should be provided in workshop materials. If not available, create custom panels for the metrics listed above.

== Exercise 3: Run GuideLLM benchmarks

Now establish your performance baseline by running comprehensive benchmarks with GuideLLM from the terminal.

=== Get the inference URL

First, retrieve your InferenceService URL:

[source,bash]
----
# Get the inference URL
INFERENCE_URL=$(oc get inferenceservice llama-vllm-single -o jsonpath='{.status.url}')
echo "INFERENCE_URL: ${INFERENCE_URL}"

# Verify the endpoint is accessible
curl -k ${INFERENCE_URL}/v1/models
----

=== Run baseline benchmark

Run a benchmark to establish your single-GPU baseline performance:

[source,bash]
----
# Run baseline benchmark with sweep profile
guidellm benchmark \
  --target "${INFERENCE_URL}" \
  --profile sweep \
  --max-seconds 60 \
  --processor meta-llama/Meta-Llama-3.1-8B-Instruct \
  --data "prompt_tokens=256,output_tokens=128"
----

**What this benchmark does**:
* `--profile sweep`: Runs multiple load levels to find saturation point
* `--max-seconds 60`: Runs for 60 seconds per load level
* `--processor`: Specifies the tokenizer for accurate token counting
* `--data`: Generates synthetic prompts with 256 input tokens and 128 output tokens

=== Monitor in Grafana

While the benchmark runs:

1. Open your vLLM Grafana dashboard
2. Watch metrics in real-time:
   * Requests per second climbing to saturation
   * GPU cache usage increasing
   * Active requests matching concurrency settings
3. Note the P95 TTFT as load increases

=== Analyze results

GuideLLM will output performance metrics including:

* **Throughput**: Requests per second at various load levels
* **TTFT (Time to First Token)**: P50, P95, P99 latencies
* **ITL (Inter-Token Latency)**: Token generation speed

Record your P95 TTFT values - you'll compare them against scaled deployments in Module 3 and llm-d in Module 4.

=== Troubleshooting

**Issue**: GuideLLM benchmarks fail with connection errors

**Solution**: Verify the vLLM endpoint is accessible:
[source,bash]
----
# Test the endpoint
INFERENCE_URL=$(oc get inferenceservice llama-vllm-single -o jsonpath='{.status.url}')
curl -k ${INFERENCE_URL}/v1/models
----

**Issue**: "processor not found" error

**Solution**: Ensure you have access to the tokenizer:
[source,bash]
----
# If using a gated model, set HF_TOKEN
export HF_TOKEN=your_token_here
----

== Module 2 summary

**What you accomplished:**
* Deployed Llama 3.1 8B Instruct using KServe and vLLM on single GPU
* Configured Grafana monitoring for vLLM metrics
* Ran GuideLLM benchmark to establish baseline performance
* Recorded your baseline P95 TTFT under load

**Key takeaways:**
* Single GPU has limited capacity before latency degrades
* Tail latency (P95/P99) degrades significantly when approaching saturation
* vLLM metrics in Grafana provide real-time performance visibility
* Your most frustrated users (P95/P99) experience significantly higher latency than median

**ParasolCloud's baseline:**
* Record your baseline P95 TTFT - you'll compare this in later modules
* Key question: Will adding more GPUs reduce tail latency?

**Next steps:**
Module 3 will scale your vLLM deployment to 4 GPUs using naive horizontal scaling. You'll discover a critical insight: **more GPUs increase capacity but don't fix tail latency**.
