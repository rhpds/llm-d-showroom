= Module 2: Deploy Model with vLLM & Benchmark
:source-highlighter: rouge
:toc: macro
:toclevels: 2

Now it's time to establish your performance baseline. Before evaluating llm-d's improvements, you need quantifiable metrics from a single-GPU vLLM deployment.

In this module, you'll deploy Llama 3.1 8B with vLLM on a single GPU using KServe in OpenShift AI, configure monitoring, and run comprehensive benchmarks with GuideLLM.

== Learning objectives
By the end of this module, you'll be able to:

* Deploy a generative AI model using KServe and vLLM ServingRuntime in OpenShift AI
* Configure Grafana dashboard to collect vLLM performance metrics
* Execute GuideLLM benchmarks to measure TTFT, ITL, and throughput
* Analyze results to identify single-instance performance limits

== Exercise 1: Deploy vLLM InferenceService

ParasolCloud needs a baseline measurement of single-GPU inference performance. You'll deploy Llama 3.1 8B Instruct on one GPU to establish this baseline.

=== Verify your environment

First, confirm your OpenShift AI environment is ready:

[.console-input]
[source,bash]
----
# Verify you're logged into OpenShift
oc whoami

# Select your lab namespace
oc project llm-d-project

# Verify GPU node is available
oc get nodes -l nvidia.com/gpu.present=true
----

Example output:

[.console-output]
[source,bash]
----
system:admin
Now using project "llm-d-project" on server "https://api.cluster-fnj7b.fnj7b.sandbox5571.opentlc.com:6443".
NAME                                        STATUS   ROLES                         AGE   VERSION
ip-10-0-47-154.us-east-2.compute.internal   Ready    control-plane,master,worker   24h   v1.33.5
----

=== Create the vLLM ServingRuntime

KServe uses ServingRuntimes to define how models are served. You'll create a vLLM-based ServingRuntime configuration.

Create the ServingRuntime for Red Hat AI Inference Server.

[.console-input]
[source,bash]
----
cat << 'EOF' | oc apply -f-
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  annotations:
    opendatahub.io/apiProtocol: REST
    opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
    opendatahub.io/runtime-version: v3.2.4
    opendatahub.io/serving-runtime-scope: global
    openshift.io/display-name: RHAIIS (vLLM) NVIDIA GPU ServingRuntime for KServe
    openshift.io/template-display-name: RHAIIS (vLLM) NVIDIA GPU ServingRuntime for KServe
    opendatahub.io/template-name: rhaiis-cuda-template
  labels:
    opendatahub.io/dashboard: "true"
  name: rhaiis-cuda
spec:
  annotations:
    prometheus.io/path: /metrics
    prometheus.io/port: "8080"
  containers:
  - args:
    - --port=8080
    - --model=/mnt/models
    - --served-model-name={{.Name}}
    command:
    - python
    - -m
    - vllm.entrypoints.openai.api_server
    env:
    - name: HF_HOME
      value: /tmp/hf_home
    - name: HF_HUB_OFFLINE
      value: "1"
    - name: VLLM_NO_USAGE_STATS
      value: "1"
    image: registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.4
    name: kserve-container
    ports:
    - containerPort: 8080
      protocol: TCP
  multiModel: false
  supportedModelFormats:
  - autoSelect: true
    name: vLLM
EOF
----

Expected output:

[.console-output]
[source,bash]
----
servingruntime.serving.kserve.io/rhaiis-cuda created
----

=== Create the InferenceService

Now deploy the Llama 3.1 8B model using the vLLM runtime.

Create the InferenceService configuration:

[.console-input]
[source,bash]
----
cat << 'EOF' | oc apply -f-
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llama-vllm-single
  annotations:
    openshift.io/display-name: Llama 3.1 8B FP8 Single
    security.opendatahub.io/enable-auth: "false"
    serving.kserve.io/deploymentMode: RawDeployment
    opendatahub.io/hardware-profile-name: accelerated-profile
    opendatahub.io/hardware-profile-namespace: redhat-ods-applications
  labels:
    networking.kserve.io/visibility: exposed
spec:
  predictor:
    automountServiceAccountToken: false
    maxReplicas: 1
    minReplicas: 1
    deploymentStrategy:
      type: Recreate
    model:
      modelFormat:
        name: vLLM
      name: ''
      resources:
        limits:
          cpu: '8'
          memory: 16Gi
          nvidia.com/gpu: '1'
        requests:
          cpu: '2'
          memory: 8Gi
          nvidia.com/gpu: '1'
      runtime: rhaiis-cuda
      storageUri: 'oci://registry.redhat.io/rhelai1/modelcar-llama-3-1-8b-instruct-fp8-dynamic:1.5'
      args:
        - --max-model-len=65536
EOF
----

Expected output:

[.console-output]
[source,bash]
----
inferenceservice.serving.kserve.io/llama-vllm-single created
----

=== Monitor deployment progress

Watch the InferenceService status:

[.console-input]
[source,bash]
----
# Watch InferenceService status, use Ctrl+C to cancel when it shows ready
oc get inferenceservice llama-vllm-single -w
----

Example output:

[.console-output]
[source,bash]
----
NAME                URL                                                                                        READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION   AGE
llama-vllm-single   https://llama-vllm-single-llm-d-project.apps.cluster-fnj7b.fnj7b.sandbox5571.opentlc.com   False                                                                 4s
llama-vllm-single   https://llama-vllm-single-llm-d-project.apps.cluster-fnj7b.fnj7b.sandbox5571.opentlc.com   True                                                                  98s
----

The deployment will progress to `READY` being `True` after the model is loaded. You can press `Ctrl+C` to cancel once it does.
The model and serving runtime should already have been downloaded to your node, speeding this up.

=== Test the deployment

It's time to make sure your model behaves as expected. First, recover the URL:

[.console-input]
[source,bash]
----
# Get the inference URL
INFERENCE_URL=$(oc get inferenceservice llama-vllm-single -o jsonpath='{.status.url}')
echo $INFERENCE_URL
----

Example output:
[.console-output]
[source,bash]
----
https://llama-vllm-single-llm-d-project.apps.cluster-fnj7b.fnj7b.sandbox5571.opentlc.com
----

Then, make sure the models endpoint returns the model name we expect:

[.console-input]
[source,bash]
----
# Check the list of models to confirm the model name is llama-vllm-single, matching the InferenceService
curl ${INFERENCE_URL}/v1/models | jq .
----

Example output:

[.console-output]
[source,bash]
----
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   479  100   479    0     0  16517      0 --:--:-- --:--:-- --:--:-- 17740
{
  "object": "list",
  "data": [
    {
      "id": "llama-vllm-single",
      "object": "model",
      "created": 1764775080,
      "owned_by": "vllm",
      "root": "/mnt/models",
      "parent": null,
      "max_model_len": 65536,
      "permission": [
        {
          "id": "modelperm-5c81cbc429a64795876e782e246ee818",
          "object": "model_permission",
          "created": 1764775080,
          "allow_create_engine": false,
          "allow_sampling": true,
          "allow_logprobs": true,
          "allow_search_indices": false,
          "allow_view": true,
          "allow_fine_tuning": false,
          "organization": "*",
          "group": null,
          "is_blocking": false
        }
      ]
    }
  ]
}
----

Now, make sure the model is able to perform inference at one of the OpenAI-compatible endpoints:

[.console-input]
[source,bash]
----
# Send a test request to the model
curl ${INFERENCE_URL}/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama-vllm-single",
    "prompt": "San Francisco is a",
    "max_tokens": 50,
    "temperature": 0.7
  }' | jq .
----

Example output:

[.console-output]
[source,bash]
----
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   818  100   698  100   120    388     66  0:00:01  0:00:01 --:--:--   455
{
  "id": "cmpl-3e58a92a36bd4336b767a26bb089d0df",
  "object": "text_completion",
  "created": 1764775084,
  "model": "llama-vllm-single",
  "choices": [
    {
      "index": 0,
      "text": " top tourist destination in the United States, attracting millions of visitors each year. From its iconic Golden Gate Bridge to its vibrant neighborhoods like Haight-Ashbury and Fisherman's Wharf, there's no shortage of things to see and do in this",
      "logprobs": null,
      "finish_reason": "length",
      "stop_reason": null,
      "token_ids": null,
      "prompt_logprobs": null,
      "prompt_token_ids": null
    }
  ],
  "service_tier": null,
  "system_fingerprint": null,
  "usage": {
    "prompt_tokens": 5,
    "total_tokens": 55,
    "completion_tokens": 50,
    "prompt_tokens_details": null
  },
  "kv_transfer_params": null
}
----

So, our single vLLM instance is up and running our Llama model successfully!

== Exercise 2: Configure Grafana monitoring

To understand vLLM performance, you need metrics visualization. You'll configure a Grafana dashboard to track vLLM inference metrics.

=== Expose vLLM metrics endpoint

vLLM exposes Prometheus metrics on its metrics port. Verify the service is exposing metrics:

[.console-input]
[source,bash]
----
# Find the vLLM pod
POD_NAME=$(oc get pod -l serving.kserve.io/inferenceservice=llama-vllm-single -oname)
echo $POD_NAME

# Port-forward to test metrics endpoint
oc port-forward ${POD_NAME} 8080:8080 &
----

[.console-input]
[source,bash]
----
# Fetch metrics
curl http://localhost:8080/metrics | grep vllm | less
----

You should see a lot of vLLM-specific metrics, including:
* `vllm:num_requests_running`
* `vllm:num_requests_waiting`
* `vllm:kv_cache_usage_perc`
* `vllm:generation_tokens_total`

[.console-input]
[source,bash]
----
# Stop port-forward when ready
pkill -f "port-forward"
----

=== Create ServiceMonitor for Prometheus

To collect vLLM metrics in Prometheus, create a ServiceMonitor:

[.console-input]
[source,bash]
----
cat << 'EOF' | oc apply -f-
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: vllm-metrics
spec:
  selector:
    matchLabels:
      serving.kserve.io/inferenceservice: llama-vllm-single
  endpoints:
    - port: http
      path: /metrics
      interval: 30s
EOF
----

=== Access Grafana dashboard

Your workshop environment should have a pre-configured Grafana instance. Access it:

[.console-input]
[source,bash]
----
# Get Grafana route
oc get route grafana-route -n grafana

# Open in browser (URL from command above)
----

Login with your OpenShift credentials.

=== Import vLLM dashboard

Import the vLLM monitoring dashboard:

1. Navigate to **Dashboards â†’ Import**
2. Upload the provided `vllm-dashboard.json` or paste the JSON content
3. Select your Prometheus data source
4. Click **Import**

Key panels to monitor:
* **Requests per second**: Current throughput
* **Active requests**: Concurrent requests being processed
* **GPU cache usage**: Memory utilization
* **Generation throughput**: Tokens generated per second
* **Request latency**: P50, P95, P99 latencies

NOTE: Dashboard files should be provided in workshop materials. If not available, create custom panels for the metrics listed above.

== Exercise 3: Run GuideLLM benchmarks

Now establish your performance baseline by running comprehensive benchmarks with GuideLLM from the terminal.

=== Get the inference URL

First, retrieve your InferenceService URL:

[.console-input]
[source,bash]
----
# Get the inference URL
INFERENCE_URL=$(oc get inferenceservice llama-vllm-single -o jsonpath='{.status.url}')
echo "INFERENCE_URL: ${INFERENCE_URL}"

# Verify the endpoint is accessible
curl -k ${INFERENCE_URL}/v1/models
----

=== Run baseline benchmark

Run a benchmark to establish your single-GPU baseline performance:

[.console-input]
[source,bash]
----
# Run baseline benchmark with sweep profile
guidellm benchmark \
  --target "${INFERENCE_URL}" \
  --profile sweep \
  --max-seconds 60 \
  --processor meta-llama/Meta-Llama-3.1-8B-Instruct \
  --data "prompt_tokens=256,output_tokens=128"
----

**What this benchmark does**:
* `--profile sweep`: Runs multiple load levels to find saturation point
* `--max-seconds 60`: Runs for 60 seconds per load level
* `--processor`: Specifies the tokenizer for accurate token counting
* `--data`: Generates synthetic prompts with 256 input tokens and 128 output tokens

=== Monitor in Grafana

While the benchmark runs:

1. Open your vLLM Grafana dashboard
2. Watch metrics in real-time:
   * Requests per second climbing to saturation
   * GPU cache usage increasing
   * Active requests matching concurrency settings
3. Note the P95 TTFT as load increases

=== Analyze results

GuideLLM will output performance metrics including:

* **Throughput**: Requests per second at various load levels
* **TTFT (Time to First Token)**: P50, P95, P99 latencies
* **ITL (Inter-Token Latency)**: Token generation speed

Record your P95 TTFT values - you'll compare them against scaled deployments in Module 3 and llm-d in Module 4.

=== Troubleshooting

**Issue**: GuideLLM benchmarks fail with connection errors

**Solution**: Verify the vLLM endpoint is accessible:
[.console-input]
[source,bash]
----
# Test the endpoint
INFERENCE_URL=$(oc get inferenceservice llama-vllm-single -o jsonpath='{.status.url}')
curl -k ${INFERENCE_URL}/v1/models
----

**Issue**: "processor not found" error

**Solution**: Ensure you have access to the tokenizer:
[.console-input]
[source,bash]
----
# If using a gated model, set HF_TOKEN
export HF_TOKEN=your_token_here
----

== Module 2 summary

**What you accomplished:**
* Deployed Llama 3.1 8B Instruct using KServe and vLLM on single GPU
* Configured Grafana monitoring for vLLM metrics
* Ran GuideLLM benchmark to establish baseline performance
* Recorded your baseline P95 TTFT under load

**Key takeaways:**
* Single GPU has limited capacity before latency degrades
* Tail latency (P95/P99) degrades significantly when approaching saturation
* vLLM metrics in Grafana provide real-time performance visibility
* Your most frustrated users (P95/P99) experience significantly higher latency than median

**ParasolCloud's baseline:**
* Record your baseline P95 TTFT - you'll compare this in later modules
* Key question: Will adding more GPUs reduce tail latency?

**Next steps:**
Module 3 will scale your vLLM deployment to 4 GPUs using naive horizontal scaling. You'll discover a critical insight: **more GPUs increase capacity but don't fix tail latency**.
