= Workshop Complete
:source-highlighter: rouge
:page-pagination!:

Congratulations on completing the llm-d workshop!

== What you demonstrated

You proved that **intelligent cache-aware routing reduces tail latency** for your most frustrated users:

* **Single GPU baseline**: Established P95/P99 latency under realistic multi-turn workloads
* **Naive 4 GPU scaling**: More capacity, but tail latency remained high due to cache misses
* **llm-d with 4 GPUs**: Cache-aware routing dramatically improved P95/P99 latency

The key insight: **more GPUs improve capacity, but intelligent routing improves customer experience**.

== When to use llm-d

Prefix-aware routing with llm-d provides the most value when your workload has:

* Shared prefixes (system prompts, common contexts)
* Multi-turn conversations with context reuse
* Tail latency requirements that matter for customer experience

== Continue learning

Explore these resources to deepen your understanding:

* **https://llm-d.ai/[llm-d project]** - Official documentation and community
* **https://github.com/vllm-project/vllm[vLLM]** - The inference engine powering the backends
* **https://github.com/vllm-project/guidellm/tree/main/docs[GuideLLM documentation]** - Benchmarking tool for LLM inference
* **https://github.com/rh-aiservices-bu/multi-turn-benchmarking[Multi-turn benchmarking tool]** - The custom benchmark used in this workshop

== Feedback

We'd love to hear about your experience with this workshop and how you plan to use llm-d in your environment.
